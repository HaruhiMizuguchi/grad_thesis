{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dboWTKluS6US"},"outputs":[],"source":["import sys\n","sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/function/myfunc_baseline\")\n","sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/function/myfunc_baseline/dataset\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qy6Cy5h7Su16"},"outputs":[],"source":["import torch\n","import time\n","import os\n","import torch.optim as optim\n","import numpy as np\n","import dataset\n","import csv\n","from model import LinearNet,Network\n","from dataset import get_loader\n","from scipy.io import savemat\n","from criteria import hLoss, rLoss, oError, Conv, avgPre\n","\n","device = ('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","def TablePlot(df,w,h,outputPath):\n","    fig, ax = plt.subplots(figsize=(w,h))\n","    ax.axis('off')\n","    ax.table(\n","        df.values,\n","        #colLabels = df.columns,\n","        loc = 'center',\n","        bbox=[0,0,1,1],\n","        fontsize = 100\n","    )\n","    plt.savefig(outputPath)\n","    plt.show()\n","def criterion(predictions, confidence):\n","\n","    assert predictions.shape == confidence.shape\n","\n","    N, C = confidence.shape\n","\n","    loss_sum = torch.zeros(N, 1, dtype=torch.float, device=device)\n","\n","    ones = torch.ones_like(confidence, dtype=torch.float, device=device)\n","    zeros = torch.zeros_like(confidence, dtype=torch.float, device=device)\n","\n","    for i in range(C):\n","        confidence_i = confidence[:, i].view(N, -1)\n","        prediction_i = predictions[:, i].view(N, -1)\n","\n","        loss = torch.max(zeros, confidence_i - confidence) * torch.max(zeros, ones - (prediction_i - predictions))\n","        loss_sum += torch.sum(loss, dim=-1, keepdim=True) / ((C - 1) * 1.0)\n","\n","    return loss_sum.mean()\n","\n","def test(net, test_loader):\n","\n","    ######################################\n","    #信頼度のしきい値\n","    threshold = 0.8\n","    ######################################\n","\n","    net.eval()\n","\n","    hLoss_list = []\n","    rLoss_list = []\n","    oError_list = []\n","    conv_list = []\n","    avgPre_list = []\n","\n","    for itr, (inputs, labels) in enumerate(test_loader):\n","\n","        inputs = inputs.to(device)\n","        outputs = net(inputs)\n","        prelabel = (torch.sigmoid(outputs) > threshold).float()\n","\n","        hLoss_list.append(hLoss(outputs, labels))\n","        rLoss_list.append(rLoss(outputs, labels))\n","        oError_list.append(oError(outputs, labels))\n","        conv_list.append(Conv(outputs, labels))\n","        avgPre_list.append(avgPre(outputs, labels))\n","\n","    hamming_loss = np.mean(hLoss_list)\n","    ranking_loss = np.mean(rLoss_list)\n","    one_error = np.mean(oError_list)\n","    coverage = np.mean(conv_list)\n","    avg_precision = np.mean(avgPre_list)\n","\n","    return hamming_loss, ranking_loss, one_error, coverage, avg_precision, outputs.cpu().detach().numpy() ,prelabel.cpu().numpy()\n","\n","def train_common(net, optimizer, train_loader):\n","    train_loss = 0\n","\n","    for batch_idx, (inputs, labels, creds) in enumerate(train_loader):\n","        net.train()\n","        inputs, labels, creds = inputs.to(device), labels.to(device), creds.to(device)\n","        outputs = net(inputs)\n","        loss = criterion(outputs, creds)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    return train_loss\n","\n","def train_meta(net, optimizer, train_loader, meta_loader):\n","    train_loss = 0.0\n","    meta_loss = 0.0\n","    clean_iter = iter(meta_loader)\n","    for batch_idx, (inputs, labels,creds) in enumerate(train_loader):\n","        inputs, labels,creds = inputs.to(device), labels.to(device),creds.to(device)\n","        meta_net = LinearNet(num_inputs=features_num, num_outputs=labels_num)\n","        meta_net.load_state_dict(net.state_dict())\n","        meta_net.to(device)\n","        epsilon = torch.zeros_like(labels, requires_grad=True)\n","\n","        y_f_hat = meta_net(inputs)\n","        l_f_meta = criterion(y_f_hat, epsilon)\n","        meta_net.zero_grad()\n","\n","        grads = torch.autograd.grad(l_f_meta, (meta_net.params()), create_graph=True)\n","        meta_net.update_params(lr, source_params=grads)\n","\n","        try:\n","            val_data, val_labels,val_creds = next(clean_iter)\n","        except StopIteration:\n","            clean_iter = iter(meta_loader)\n","            val_data, val_labels,val_creds = next(clean_iter)\n","\n","        val_data, val_labels = val_data.to(device), val_labels.to(device)\n","\n","        y_g_hat = meta_net(val_data)\n","        l_g_meta = criterion(y_g_hat, val_labels)\n","\n","        grad_eps = torch.autograd.grad(l_g_meta, epsilon, only_inputs=True)[0]\n","\n","        # computing and normalizing the confidence matrix P\n","        # p_tilde = torch.clamp(epsilon - grad_eps, min=0)\n","        p_tilde = torch.clamp(-grad_eps, min=0)\n","        p_tilde *= labels\n","\n","        max_row = torch.max(p_tilde, dim=1, keepdim=True)[0]\n","        ones = torch.ones_like(max_row)\n","        max_row = torch.where(max_row == 0, ones, max_row)\n","        p = p_tilde / max_row\n","        # p.shape = batch_size * class_num\n","\n","        y_f_hat = net(inputs)\n","        l_f = criterion(y_f_hat, p)\n","\n","        optimizer.zero_grad()\n","        l_f.backward()\n","        optimizer.step()\n","\n","        meta_loss += l_g_meta.item()\n","        train_loss += l_f.item()\n","\n","    return meta_loss, train_loss\n","\n","\n","def baseline(train_loader, test_loader, meta_loader, num_epoch=300, clean=True):\n","    # method = baseline for baseline with meta data, else for baseline without meta data and ground-truth\n","\n","    net = LinearNet(num_inputs=features_num, num_outputs=labels_num)\n","    #net = Network(num_inputs=features_num,num_hides = 5,num_outputs=labels_num)\n","    net = net.to(device)\n","    optimizer = optim.SGD(params=net.params(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","\n","    hLoss_list = []\n","    rLoss_list = []\n","    oError_list = []\n","    conv_list = []\n","    avgPre_list = []\n","\n","    for epoch in range(num_epoch):\n","        loss = 0.0\n","        # adjust_learning_rate(optimizer, epoch, lr)\n","        if clean:\n","            loss += train_common(net, optimizer, meta_loader)\n","            loss += train_common(net, optimizer, train_loader)\n","            loss /= (len(meta_loader) + len(train_loader))\n","        else:\n","            loss += train_common(net, optimizer, train_loader)\n","            loss /= len(train_loader)\n","\n","        hamming_loss, ranking_loss, one_error, coverage, avg_precision, outputs, p_labels = test(net, test_loader)\n","\n","        print('Batch: [{:0>4}/{:0>4}] '.format(epoch + 1, num_epoch),\n","              'training loss: {:.4f} '.format(loss),\n","              'hLoss: {:.4f} '.format(hamming_loss),\n","              'rLoss: {:.4f} '.format(ranking_loss),\n","              'oError: {:.4f} '.format(one_error),\n","              'conv: {:.4f} '.format(coverage),\n","              'avgPre: {:.4f}'.format(avg_precision))\n","\n","    #最終的な結果と出力と予測ラベルを返す\n","    return hamming_loss, ranking_loss, one_error, coverage, avg_precision, outputs, p_labels\n","\n","def metalearning(train_loader, test_loader, meta_loader,train_data,train_plabels, meta_features, meta_labels,data,p_true,num_epoch=300, clean=True,save_creds_every_epoch=False):\n","    \n","    net = LinearNet(num_inputs=features_num, num_outputs=labels_num)\n","    net = net.to(device)\n","    optimizer = optim.SGD(params=net.params(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","\n","    hLoss_list = []\n","    rLoss_list = []\n","    oError_list = []\n","    conv_list = []\n","    avgPre_list = []\n","\n","    for epoch in range(num_epoch):\n","        train_loss = 0.0\n","        meta_loss = 0.0\n","\n","        if clean:\n","            train_loss += train_common(net, optimizer, train_loader)\n","            train_l, meta_l = train_meta(net, optimizer, train_loader, meta_loader)\n","            train_loss += train_l\n","            meta_loss += meta_l\n","\n","            train_loss /= (len(train_loader) + len(meta_loader))\n","            meta_loss /= len(train_loader)\n","        else:\n","            train_l, meta_l = train_meta(net, optimizer, train_loader, meta_loader)\n","            train_loss += train_l\n","            meta_loss += meta_l\n","\n","            train_loss /= len(train_loader)\n","            meta_loss /= len(train_loader)\n","\n","        if save_creds_every_epoch == True:\n","          p_creds = compute_p_creds(train_data,train_plabels, meta_features, meta_labels,net)\n","          np.savetxt(f\"/content/drive/MyDrive/Colab Notebooks/result/predict_creds/PMLMD_every_epoch/{data}/{str(p_true)}/{epoch}.csv\",p_creds,delimiter = \",\")\n","\n","        hamming_loss, ranking_loss, one_error, coverage, avg_precision ,outputs, p_labels= test(net, test_loader)\n","\n","        print('Batch: [{:0>4}/{:0>4}] '.format(epoch + 1, num_epoch),\n","              'training loss: {:.4f} '.format(train_loss),\n","              'meta_loss: {:.4f} '.format(meta_loss),\n","              'hLoss: {:.4f} '.format(hamming_loss),\n","              'rLoss: {:.4f} '.format(ranking_loss),\n","              'oError: {:.4f} '.format(one_error),\n","              'conv: {:.4f} '.format(coverage),\n","              'avgPre: {:.4f}'.format(avg_precision))\n","\n","    #最終的な結果と出力と予測ラベルを返す\n","    return hamming_loss, ranking_loss, one_error, coverage, avg_precision, outputs, p_labels, net\n","\n","#メタ学習時の信頼度の推定\n","def compute_p_creds(train_data,train_plabels, meta_features, meta_labels, net):\n","    # 最終的なネットワークをもとに訓練データ全体の信頼度を計算\n","    inputs = train_data\n","    labels = train_plabels\n","    inputs, labels = inputs.to(device), labels.to(device)\n","\n","    meta_net = LinearNet(num_inputs=features_num, num_outputs=labels_num)\n","    meta_net.load_state_dict(net.state_dict())\n","    meta_net.to(device)\n","    epsilon = torch.zeros_like(labels, requires_grad=True)\n","\n","    y_f_hat = meta_net(inputs)\n","    l_f_meta = criterion(y_f_hat, epsilon)\n","    meta_net.zero_grad()\n","\n","    grads = torch.autograd.grad(l_f_meta, (meta_net.params()), create_graph=True)\n","    meta_net.update_params(lr, source_params=grads)\n","\n","    val_data = meta_features\n","    val_labels = meta_labels\n","    val_data, val_labels = val_data.to(device), val_labels.to(device)\n","\n","    y_g_hat = meta_net(val_data)\n","    l_g_meta = criterion(y_g_hat, val_labels)\n","\n","    grad_eps = torch.autograd.grad(l_g_meta, epsilon, only_inputs=True)[0]\n","\n","    # computing and normalizing the confidence matrix P\n","    # p_tilde = torch.clamp(epsilon - grad_eps, min=0)\n","    p_tilde = torch.clamp(-grad_eps, min=0)\n","    p_tilde *= labels\n","\n","    max_row = torch.max(p_tilde, dim=1, keepdim=True)[0]\n","    ones = torch.ones_like(max_row)\n","    max_row = torch.where(max_row == 0, ones, max_row)\n","    p = p_tilde / max_row\n","    p = p.cpu().numpy()\n","    return p\n","    \n","def save_p_creds(data,train_target,p_noise,p_true,cv_num,creds):\n","  save_path = f\"/content/drive/MyDrive/Colab Notebooks/result/predict_creds/PML-MD/{data}/{str(p_noise)}/true/{str(p_true)}/\"\n","  np.savetxt(save_path+\"creds_\"+str(cv_num)+\".csv\",creds,delimiter=\",\")\n","  np.savetxt(save_path+\"target_\"+str(cv_num)+\".csv\",train_target,delimiter=\",\")\n","\n","def save_result(data, method, learning_rate, result):\n","    file = './result/LinearNet/' + data\n","    if not os.path.exists(file):\n","        os.mkdir(file)\n","    filename = './result/LinearNet/' + data + '/' + method + '_' + str(learning_rate) + '.mat'\n","    content = {}\n","\n","    content['result'] = result\n","    savemat(filename, content)\n","\n","def save_log(content, learning_rate):\n","    filename = './result/LinearNet/' + str(learning_rate) + '_log.txt'\n","    file = open(filename, 'a')\n","    file.write(content)\n","    file.close()\n","\n","def adjust_learning_rate(optimizer, epochs, learning_rate):\n","    lr = learning_rate * ((0.1 ** int(epochs >= 80)) * (0.1 ** int(epochs >= 100)))  # For WRN-28-10\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","# 訓練データを取る\n","def get_train_data(dataname,p_noise,p_true,cv_num):\n","  data_path = f\"/content/drive/MyDrive/Colab Notebooks/new_data2/\" + dataname + \"/\"\n","  features = np.loadtxt(data_path+\"data.csv\", delimiter=',')\n","  labels = np.loadtxt(data_path+\"target.csv\", delimiter=',',dtype = float)\n","  if dataname in ['mirflickr', 'music_emotion', 'music_style','YeastBP']:\n","    plabels = np.loadtxt(data_path+\"cand/0.csv\", delimiter=',',dtype = float)\n","  else:\n","    plabels = np.loadtxt(data_path+\"cand/\"+str(p_noise)+\".csv\", delimiter=',',dtype = float)\n","  cv_inds = np.loadtxt(data_path+\"index/5-cv.csv\",delimiter=',',dtype = int)-1\n","  data_path_true = \"/content/drive/MyDrive/Colab Notebooks/new_data/\" + dataname + \"/index/true/\" + str(p_true) +  \"/A.csv\"\n","  with open(data_path_true) as f:\n","      reader = csv.reader(f)\n","      l = [row for row in reader]\n","  meta_inds = [[int(v) for v in row] for row in l]\n","\n","  #全体の特徴をtensorに変換\n","  features = torch.tensor(features, dtype=torch.float)\n","  labels = torch.tensor(labels, dtype=torch.float)\n","  plabels = torch.tensor(plabels, dtype=torch.float)\n","\n","  zeros = torch.zeros_like(labels)\n","  labels = torch.where(labels == -1, zeros, labels)\n","  plabels = torch.where(plabels == -1, zeros, plabels)\n","\n","  #訓練とテストに分割(自分は5分割交差検証)\n","  # split training into train and test set\n","  \"\"\"n = len(features)\n","  train_size = int(n * prec)\"\"\"\n","  \n","  #trainとtestとmetaのインデックス(自分はcv_inds)\n","  \"\"\"indices = torch.randperm(n)\n","  train_idxs = indices[:train_size]\n","  test_idxs = indices[train_size:]\"\"\"\n","  index = np.where(cv_inds!=cv_num)\n","  train_inds = np.ravel(index[0])\n","  train_idxs = torch.tensor(train_inds)\n","  \n","  index = np.where(cv_inds==cv_num)\n","  index = np.ravel(index[0])\n","  test_idxs = torch.tensor(index)\n","  \n","\n","  #上で指定したインデックスでtrainとtestに分ける             \n","  train_features = torch.index_select(features, 0, train_idxs)\n","  train_labels = torch.index_select(labels, 0, train_idxs)\n","  train_plabels = torch.index_select(plabels, 0, train_idxs)\n","\n","  clean_features = train_features.clone()\n","  clean_labels = train_labels.clone()\n","\n","  noisy_features = train_features.clone()\n","  noisy_labels = train_plabels.clone()\n","\n","  test_features = torch.index_select(features, 0, test_idxs)\n","  test_labels = torch.index_select(labels, 0, test_idxs)\n","\n","  \n","  #検証集合を作る\n","  # select a clean batch from training set\n","  meta_idxs = np.array([],dtype = \"int\")\n","  for i in range(5):\n","    if i != cv_num:\n","      meta_idxs = np.append(meta_idxs,np.array(meta_inds[i],dtype=\"int\"))\n","\n","  if dataname in [\"music_emotion\",\"music_style\"]:\n","    batch_size = 200\n","  elif dataname in [\"mirflickr\"]:\n","    batch_size = 500\n","  elif dataname in ['CAL500','emotions','genbase']:\n","    batch_size = 50\n","  elif dataname in ['scene','enron']:\n","    batch_size = 100\n","\n","  train_idxs = np.setdiff1d(train_inds,meta_idxs)\n","  train_idxs = torch.tensor(train_idxs)\n","  meta_idxs = torch.tensor(meta_idxs)\n","\n","  meta_features = torch.index_select(features, 0, meta_idxs)\n","  meta_labels = torch.index_select(labels, 0, meta_idxs)\n","  \n","  train_features = torch.index_select(features, 0, train_idxs)\n","  train_target = torch.index_select(labels, 0, train_idxs)\n","  train_plabels = torch.index_select(plabels, 0, train_idxs)\n","  \n","  return train_features, train_target,  train_plabels, meta_features, meta_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671798365127,"user":{"displayName":"水口晴陽","userId":"02563633021533009649"},"user_tz":-540},"id":"85aHyNlQYOew","outputId":"997981d0-a280-4cb1-a4d7-2322362aa816"},"outputs":[{"data":{"text/plain":["<module 'dataset' from '/content/drive/MyDrive/Colab Notebooks/function/myfunc_baseline/dataset.py'>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["import importlib\n","importlib.reload(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":488580,"status":"ok","timestamp":1674196376744,"user":{"displayName":"水口晴陽","userId":"02563633021533009649"},"user_tz":-540},"id":"ki53ybwnYMm8","outputId":"4c3d59a2-86fd-4eb5-b42b-98a2b61e8cff"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","0\n","data=music_emotion/cv=0/p_noise=0/p_true=1/method=meta\n","\n","W:(5412, 54)\n","train:(5412, 98)\n","meta:(54, 98)\n","Batch: [0001/0300]  training loss: 1.5716  meta_loss: 2.1236  hLoss: 0.2198  rLoss: 0.3401  oError: 0.6650  conv: 0.5115  avgPre: 0.4960\n","Batch: [0002/0300]  training loss: 1.2881  meta_loss: 1.5424  hLoss: 0.2174  rLoss: 0.2879  oError: 0.5106  conv: 0.4605  avgPre: 0.5780\n","Batch: [0003/0300]  training loss: 1.2328  meta_loss: 1.6842  hLoss: 0.2120  rLoss: 0.2943  oError: 0.5604  conv: 0.4628  avgPre: 0.5611\n","Batch: [0004/0300]  training loss: 1.1905  meta_loss: 1.4682  hLoss: 0.2174  rLoss: 0.2739  oError: 0.5669  conv: 0.4437  avgPre: 0.5699\n","Batch: [0005/0300]  training loss: 1.1748  meta_loss: 1.5445  hLoss: 0.2188  rLoss: 0.2754  oError: 0.5443  conv: 0.4359  avgPre: 0.5707\n","Batch: [0006/0300]  training loss: 1.1273  meta_loss: 1.4998  hLoss: 0.2183  rLoss: 0.2758  oError: 0.5026  conv: 0.4411  avgPre: 0.5872\n","Batch: [0007/0300]  training loss: 1.1237  meta_loss: 1.5394  hLoss: 0.2186  rLoss: 0.2828  oError: 0.5026  conv: 0.4537  avgPre: 0.5736\n","Batch: [0008/0300]  training loss: 1.1112  meta_loss: 1.5482  hLoss: 0.2137  rLoss: 0.2734  oError: 0.5018  conv: 0.4424  avgPre: 0.5861\n","Batch: [0009/0300]  training loss: 1.1154  meta_loss: 1.4981  hLoss: 0.2153  rLoss: 0.2733  oError: 0.4872  conv: 0.4418  avgPre: 0.5901\n","Batch: [0010/0300]  training loss: 1.1093  meta_loss: 1.4813  hLoss: 0.2179  rLoss: 0.2597  oError: 0.4733  conv: 0.4351  avgPre: 0.6051\n","Batch: [0011/0300]  training loss: 1.0785  meta_loss: 1.5259  hLoss: 0.2184  rLoss: 0.2737  oError: 0.5699  conv: 0.4362  avgPre: 0.5690\n","Batch: [0012/0300]  training loss: 1.0926  meta_loss: 1.5082  hLoss: 0.2119  rLoss: 0.2628  oError: 0.4813  conv: 0.4323  avgPre: 0.6110\n","Batch: [0013/0300]  training loss: 1.0814  meta_loss: 1.5789  hLoss: 0.2069  rLoss: 0.2574  oError: 0.4601  conv: 0.4301  avgPre: 0.6194\n","Batch: [0014/0300]  training loss: 1.0945  meta_loss: 1.6240  hLoss: 0.2186  rLoss: 0.2652  oError: 0.5267  conv: 0.4287  avgPre: 0.5712\n","Batch: [0015/0300]  training loss: 1.1233  meta_loss: 1.6173  hLoss: 0.2190  rLoss: 0.2695  oError: 0.5106  conv: 0.4246  avgPre: 0.5926\n","Batch: [0016/0300]  training loss: 1.0647  meta_loss: 1.4973  hLoss: 0.2171  rLoss: 0.2537  oError: 0.4543  conv: 0.4186  avgPre: 0.6183\n","Batch: [0017/0300]  training loss: 1.0669  meta_loss: 1.5426  hLoss: 0.2105  rLoss: 0.2559  oError: 0.5187  conv: 0.4203  avgPre: 0.5923\n","Batch: [0018/0300]  training loss: 1.0790  meta_loss: 1.5978  hLoss: 0.2186  rLoss: 0.2488  oError: 0.5516  conv: 0.4145  avgPre: 0.5918\n","Batch: [0019/0300]  training loss: 1.0460  meta_loss: 1.5342  hLoss: 0.2181  rLoss: 0.2619  oError: 0.5699  conv: 0.4302  avgPre: 0.5753\n","Batch: [0020/0300]  training loss: 1.0308  meta_loss: 1.4557  hLoss: 0.2139  rLoss: 0.2654  oError: 0.5172  conv: 0.4339  avgPre: 0.5883\n","Batch: [0021/0300]  training loss: 1.0673  meta_loss: 1.6683  hLoss: 0.2170  rLoss: 0.2514  oError: 0.4667  conv: 0.4216  avgPre: 0.6149\n","Batch: [0022/0300]  training loss: 1.0380  meta_loss: 1.5243  hLoss: 0.2158  rLoss: 0.2421  oError: 0.4916  conv: 0.4097  avgPre: 0.6177\n","Batch: [0023/0300]  training loss: 1.0382  meta_loss: 1.5256  hLoss: 0.2139  rLoss: 0.2571  oError: 0.5143  conv: 0.4271  avgPre: 0.5959\n","Batch: [0024/0300]  training loss: 1.0187  meta_loss: 1.4495  hLoss: 0.2175  rLoss: 0.2484  oError: 0.4733  conv: 0.4166  avgPre: 0.6201\n","Batch: [0025/0300]  training loss: 1.0117  meta_loss: 1.4595  hLoss: 0.2175  rLoss: 0.2335  oError: 0.4565  conv: 0.3990  avgPre: 0.6299\n","Batch: [0026/0300]  training loss: 1.0133  meta_loss: 1.5408  hLoss: 0.2177  rLoss: 0.2482  oError: 0.4733  conv: 0.4176  avgPre: 0.6109\n","Batch: [0027/0300]  training loss: 1.0132  meta_loss: 1.4380  hLoss: 0.2182  rLoss: 0.2537  oError: 0.5201  conv: 0.4235  avgPre: 0.5984\n","Batch: [0028/0300]  training loss: 1.0199  meta_loss: 1.5042  hLoss: 0.2171  rLoss: 0.2397  oError: 0.4645  conv: 0.4146  avgPre: 0.6310\n","Batch: [0029/0300]  training loss: 1.0300  meta_loss: 1.4894  hLoss: 0.2170  rLoss: 0.2538  oError: 0.5384  conv: 0.4216  avgPre: 0.5905\n","Batch: [0030/0300]  training loss: 1.0256  meta_loss: 1.5155  hLoss: 0.2157  rLoss: 0.2407  oError: 0.4331  conv: 0.4180  avgPre: 0.6368\n","Batch: [0031/0300]  training loss: 1.0221  meta_loss: 1.5380  hLoss: 0.2165  rLoss: 0.2512  oError: 0.4623  conv: 0.4240  avgPre: 0.6032\n","Batch: [0032/0300]  training loss: 1.0175  meta_loss: 1.4726  hLoss: 0.2179  rLoss: 0.2510  oError: 0.4762  conv: 0.4154  avgPre: 0.6067\n","Batch: [0033/0300]  training loss: 1.0230  meta_loss: 1.5214  hLoss: 0.2181  rLoss: 0.2478  oError: 0.4718  conv: 0.4215  avgPre: 0.6174\n","Batch: [0034/0300]  training loss: 1.0170  meta_loss: 1.4980  hLoss: 0.2179  rLoss: 0.2434  oError: 0.4704  conv: 0.4069  avgPre: 0.6110\n","Batch: [0035/0300]  training loss: 1.0045  meta_loss: 1.5120  hLoss: 0.2174  rLoss: 0.2481  oError: 0.4784  conv: 0.4169  avgPre: 0.6167\n","Batch: [0036/0300]  training loss: 1.0130  meta_loss: 1.4302  hLoss: 0.2129  rLoss: 0.2393  oError: 0.4740  conv: 0.4060  avgPre: 0.6255\n","Batch: [0037/0300]  training loss: 1.0306  meta_loss: 1.5976  hLoss: 0.2090  rLoss: 0.2591  oError: 0.5150  conv: 0.4400  avgPre: 0.5862\n","Batch: [0038/0300]  training loss: 1.0305  meta_loss: 1.6222  hLoss: 0.2088  rLoss: 0.2613  oError: 0.5018  conv: 0.4388  avgPre: 0.6008\n","Batch: [0039/0300]  training loss: 1.0172  meta_loss: 1.6657  hLoss: 0.2177  rLoss: 0.2423  oError: 0.5406  conv: 0.4029  avgPre: 0.5972\n","Batch: [0040/0300]  training loss: 1.0145  meta_loss: 1.5256  hLoss: 0.2140  rLoss: 0.2362  oError: 0.4236  conv: 0.4078  avgPre: 0.6438\n","Batch: [0041/0300]  training loss: 1.0021  meta_loss: 1.4938  hLoss: 0.2131  rLoss: 0.2618  oError: 0.5048  conv: 0.4325  avgPre: 0.5907\n","Batch: [0042/0300]  training loss: 1.0293  meta_loss: 1.5770  hLoss: 0.2175  rLoss: 0.2473  oError: 0.4653  conv: 0.4188  avgPre: 0.6242\n","Batch: [0043/0300]  training loss: 1.0447  meta_loss: 1.6343  hLoss: 0.2153  rLoss: 0.2631  oError: 0.5106  conv: 0.4184  avgPre: 0.5901\n","Batch: [0044/0300]  training loss: 1.0260  meta_loss: 1.5968  hLoss: 0.2162  rLoss: 0.2241  oError: 0.4316  conv: 0.3927  avgPre: 0.6453\n","Batch: [0045/0300]  training loss: 0.9990  meta_loss: 1.4557  hLoss: 0.2154  rLoss: 0.2432  oError: 0.4638  conv: 0.4127  avgPre: 0.6153\n","Batch: [0046/0300]  training loss: 0.9858  meta_loss: 1.4600  hLoss: 0.2157  rLoss: 0.2295  oError: 0.4060  conv: 0.4022  avgPre: 0.6501\n","Batch: [0047/0300]  training loss: 1.0651  meta_loss: 1.6536  hLoss: 0.2105  rLoss: 0.2455  oError: 0.4470  conv: 0.4224  avgPre: 0.6206\n","Batch: [0048/0300]  training loss: 1.0251  meta_loss: 1.6037  hLoss: 0.2097  rLoss: 0.2557  oError: 0.4813  conv: 0.4168  avgPre: 0.6042\n","Batch: [0049/0300]  training loss: 1.0369  meta_loss: 1.6004  hLoss: 0.2107  rLoss: 0.2656  oError: 0.5121  conv: 0.4393  avgPre: 0.5827\n","Batch: [0050/0300]  training loss: 1.0037  meta_loss: 1.6016  hLoss: 0.2159  rLoss: 0.2482  oError: 0.5179  conv: 0.4101  avgPre: 0.6018\n","Batch: [0051/0300]  training loss: 0.9907  meta_loss: 1.5372  hLoss: 0.2157  rLoss: 0.2294  oError: 0.4462  conv: 0.3976  avgPre: 0.6327\n","Batch: [0052/0300]  training loss: 0.9860  meta_loss: 1.4738  hLoss: 0.2159  rLoss: 0.2265  oError: 0.4470  conv: 0.3975  avgPre: 0.6401\n","Batch: [0053/0300]  training loss: 1.0217  meta_loss: 1.5710  hLoss: 0.2164  rLoss: 0.2543  oError: 0.4916  conv: 0.4211  avgPre: 0.5973\n","Batch: [0054/0300]  training loss: 0.9905  meta_loss: 1.4941  hLoss: 0.2152  rLoss: 0.2382  oError: 0.4740  conv: 0.4078  avgPre: 0.6255\n","Batch: [0055/0300]  training loss: 1.0217  meta_loss: 1.5576  hLoss: 0.2184  rLoss: 0.2564  oError: 0.5172  conv: 0.4170  avgPre: 0.5865\n","Batch: [0056/0300]  training loss: 1.0077  meta_loss: 1.5044  hLoss: 0.2128  rLoss: 0.2381  oError: 0.4535  conv: 0.4036  avgPre: 0.6249\n","Batch: [0057/0300]  training loss: 1.0006  meta_loss: 1.5498  hLoss: 0.2258  rLoss: 0.2766  oError: 0.6064  conv: 0.4353  avgPre: 0.5560\n","Batch: [0058/0300]  training loss: 1.0063  meta_loss: 1.5553  hLoss: 0.2157  rLoss: 0.2364  oError: 0.4550  conv: 0.3992  avgPre: 0.6307\n","Batch: [0059/0300]  training loss: 1.0060  meta_loss: 1.6159  hLoss: 0.2162  rLoss: 0.2577  oError: 0.5552  conv: 0.4331  avgPre: 0.5806\n","Batch: [0060/0300]  training loss: 1.0120  meta_loss: 1.5707  hLoss: 0.2148  rLoss: 0.2305  oError: 0.4492  conv: 0.4029  avgPre: 0.6359\n","Batch: [0061/0300]  training loss: 0.9840  meta_loss: 1.4675  hLoss: 0.2086  rLoss: 0.2447  oError: 0.5091  conv: 0.4022  avgPre: 0.6126\n","Batch: [0062/0300]  training loss: 0.9855  meta_loss: 1.5291  hLoss: 0.2161  rLoss: 0.2477  oError: 0.5084  conv: 0.4194  avgPre: 0.6043\n","Batch: [0063/0300]  training loss: 0.9731  meta_loss: 1.4590  hLoss: 0.2173  rLoss: 0.2445  oError: 0.5333  conv: 0.4183  avgPre: 0.5981\n","Batch: [0064/0300]  training loss: 0.9780  meta_loss: 1.4755  hLoss: 0.2090  rLoss: 0.2501  oError: 0.4740  conv: 0.4289  avgPre: 0.6153\n","Batch: [0065/0300]  training loss: 0.9790  meta_loss: 1.4456  hLoss: 0.2167  rLoss: 0.2573  oError: 0.5018  conv: 0.4223  avgPre: 0.6031\n","Batch: [0066/0300]  training loss: 0.9753  meta_loss: 1.5066  hLoss: 0.2165  rLoss: 0.2483  oError: 0.5604  conv: 0.4086  avgPre: 0.5870\n","Batch: [0067/0300]  training loss: 0.9815  meta_loss: 1.5163  hLoss: 0.2170  rLoss: 0.2575  oError: 0.5326  conv: 0.4224  avgPre: 0.5883\n","Batch: [0068/0300]  training loss: 0.9696  meta_loss: 1.5101  hLoss: 0.2181  rLoss: 0.2596  oError: 0.5040  conv: 0.4224  avgPre: 0.5931\n","Batch: [0069/0300]  training loss: 0.9822  meta_loss: 1.4982  hLoss: 0.2051  rLoss: 0.2511  oError: 0.5011  conv: 0.4156  avgPre: 0.6001\n","Batch: [0070/0300]  training loss: 0.9759  meta_loss: 1.5291  hLoss: 0.2086  rLoss: 0.2561  oError: 0.4901  conv: 0.4335  avgPre: 0.5985\n","Batch: [0071/0300]  training loss: 0.9671  meta_loss: 1.4984  hLoss: 0.2143  rLoss: 0.2374  oError: 0.4674  conv: 0.4063  avgPre: 0.6248\n","Batch: [0072/0300]  training loss: 0.9644  meta_loss: 1.4784  hLoss: 0.2116  rLoss: 0.2388  oError: 0.4433  conv: 0.4223  avgPre: 0.6250\n","Batch: [0073/0300]  training loss: 0.9858  meta_loss: 1.5073  hLoss: 0.2090  rLoss: 0.2370  oError: 0.4726  conv: 0.4057  avgPre: 0.6259\n","Batch: [0074/0300]  training loss: 1.0028  meta_loss: 1.5599  hLoss: 0.2121  rLoss: 0.2467  oError: 0.4821  conv: 0.4126  avgPre: 0.6129\n","Batch: [0075/0300]  training loss: 0.9702  meta_loss: 1.4416  hLoss: 0.2150  rLoss: 0.2440  oError: 0.5033  conv: 0.4150  avgPre: 0.6030\n","Batch: [0076/0300]  training loss: 0.9940  meta_loss: 1.5992  hLoss: 0.2169  rLoss: 0.2451  oError: 0.4704  conv: 0.4146  avgPre: 0.6201\n","Batch: [0077/0300]  training loss: 1.0020  meta_loss: 1.6105  hLoss: 0.2162  rLoss: 0.2295  oError: 0.4477  conv: 0.4023  avgPre: 0.6397\n","Batch: [0078/0300]  training loss: 0.9763  meta_loss: 1.4787  hLoss: 0.2153  rLoss: 0.2365  oError: 0.4638  conv: 0.4039  avgPre: 0.6287\n","Batch: [0079/0300]  training loss: 0.9830  meta_loss: 1.4867  hLoss: 0.2175  rLoss: 0.2407  oError: 0.4865  conv: 0.4047  avgPre: 0.6192\n","Batch: [0080/0300]  training loss: 0.9723  meta_loss: 1.4624  hLoss: 0.2167  rLoss: 0.2404  oError: 0.4931  conv: 0.4039  avgPre: 0.6117\n","Batch: [0081/0300]  training loss: 0.9863  meta_loss: 1.5197  hLoss: 0.2156  rLoss: 0.2562  oError: 0.5121  conv: 0.4183  avgPre: 0.5937\n","Batch: [0082/0300]  training loss: 0.9847  meta_loss: 1.5598  hLoss: 0.2162  rLoss: 0.2378  oError: 0.4740  conv: 0.4079  avgPre: 0.6167\n","Batch: [0083/0300]  training loss: 0.9674  meta_loss: 1.5020  hLoss: 0.2026  rLoss: 0.2538  oError: 0.4828  conv: 0.4288  avgPre: 0.6030\n","Batch: [0084/0300]  training loss: 0.9802  meta_loss: 1.5225  hLoss: 0.2074  rLoss: 0.2647  oError: 0.5282  conv: 0.4362  avgPre: 0.5717\n","Batch: [0085/0300]  training loss: 1.0234  meta_loss: 1.5281  hLoss: 0.2163  rLoss: 0.2482  oError: 0.4682  conv: 0.4210  avgPre: 0.6171\n","Batch: [0086/0300]  training loss: 1.0033  meta_loss: 1.6299  hLoss: 0.2162  rLoss: 0.2530  oError: 0.5187  conv: 0.4212  avgPre: 0.5896\n","Batch: [0087/0300]  training loss: 1.0221  meta_loss: 1.6497  hLoss: 0.2034  rLoss: 0.2459  oError: 0.4813  conv: 0.4066  avgPre: 0.6105\n","Batch: [0088/0300]  training loss: 1.0117  meta_loss: 1.6154  hLoss: 0.2171  rLoss: 0.2408  oError: 0.5069  conv: 0.4114  avgPre: 0.5987\n","Batch: [0089/0300]  training loss: 0.9843  meta_loss: 1.5466  hLoss: 0.2169  rLoss: 0.2474  oError: 0.4711  conv: 0.4217  avgPre: 0.6181\n","Batch: [0090/0300]  training loss: 0.9940  meta_loss: 1.5793  hLoss: 0.2065  rLoss: 0.2408  oError: 0.4799  conv: 0.4033  avgPre: 0.6106\n","Batch: [0091/0300]  training loss: 1.0200  meta_loss: 1.6670  hLoss: 0.2124  rLoss: 0.2367  oError: 0.4514  conv: 0.3960  avgPre: 0.6257\n","Batch: [0092/0300]  training loss: 0.9852  meta_loss: 1.5104  hLoss: 0.2157  rLoss: 0.2308  oError: 0.4331  conv: 0.3994  avgPre: 0.6447\n","Batch: [0093/0300]  training loss: 0.9736  meta_loss: 1.4239  hLoss: 0.2177  rLoss: 0.2565  oError: 0.5611  conv: 0.4212  avgPre: 0.5699\n","Batch: [0094/0300]  training loss: 0.9668  meta_loss: 1.4738  hLoss: 0.2151  rLoss: 0.2395  oError: 0.4616  conv: 0.4135  avgPre: 0.6244\n","Batch: [0095/0300]  training loss: 0.9541  meta_loss: 1.4110  hLoss: 0.2156  rLoss: 0.2477  oError: 0.4777  conv: 0.4260  avgPre: 0.6116\n","Batch: [0096/0300]  training loss: 0.9702  meta_loss: 1.4860  hLoss: 0.2436  rLoss: 0.2772  oError: 0.7498  conv: 0.4373  avgPre: 0.5106\n","Batch: [0097/0300]  training loss: 0.9956  meta_loss: 1.6048  hLoss: 0.2173  rLoss: 0.2499  oError: 0.5055  conv: 0.4216  avgPre: 0.5891\n","Batch: [0098/0300]  training loss: 0.9715  meta_loss: 1.4656  hLoss: 0.2181  rLoss: 0.2500  oError: 0.5187  conv: 0.4179  avgPre: 0.6003\n","Batch: [0099/0300]  training loss: 0.9773  meta_loss: 1.5314  hLoss: 0.2159  rLoss: 0.2480  oError: 0.4777  conv: 0.4217  avgPre: 0.6055\n","Batch: [0100/0300]  training loss: 0.9863  meta_loss: 1.4622  hLoss: 0.2171  rLoss: 0.2461  oError: 0.4828  conv: 0.4087  avgPre: 0.6128\n","Batch: [0101/0300]  training loss: 0.9936  meta_loss: 1.4700  hLoss: 0.2165  rLoss: 0.2579  oError: 0.4894  conv: 0.4314  avgPre: 0.5933\n","Batch: [0102/0300]  training loss: 0.9783  meta_loss: 1.4952  hLoss: 0.2150  rLoss: 0.2360  oError: 0.4506  conv: 0.4061  avgPre: 0.6256\n","Batch: [0103/0300]  training loss: 0.9843  meta_loss: 1.4668  hLoss: 0.2158  rLoss: 0.2406  oError: 0.4682  conv: 0.4120  avgPre: 0.6252\n","Batch: [0104/0300]  training loss: 0.9969  meta_loss: 1.5139  hLoss: 0.2085  rLoss: 0.2394  oError: 0.4711  conv: 0.4022  avgPre: 0.6149\n","Batch: [0105/0300]  training loss: 1.0064  meta_loss: 1.5533  hLoss: 0.2135  rLoss: 0.2305  oError: 0.4550  conv: 0.3918  avgPre: 0.6357\n","Batch: [0106/0300]  training loss: 0.9910  meta_loss: 1.5513  hLoss: 0.2168  rLoss: 0.2555  oError: 0.5106  conv: 0.4213  avgPre: 0.5894\n","Batch: [0107/0300]  training loss: 0.9978  meta_loss: 1.5145  hLoss: 0.2167  rLoss: 0.2432  oError: 0.4806  conv: 0.4160  avgPre: 0.6177\n","Batch: [0108/0300]  training loss: 0.9746  meta_loss: 1.4973  hLoss: 0.2169  rLoss: 0.2450  oError: 0.5216  conv: 0.4109  avgPre: 0.5988\n","Batch: [0109/0300]  training loss: 0.9625  meta_loss: 1.4603  hLoss: 0.2175  rLoss: 0.2394  oError: 0.4792  conv: 0.4034  avgPre: 0.6101\n","Batch: [0110/0300]  training loss: 0.9804  meta_loss: 1.5391  hLoss: 0.2139  rLoss: 0.2294  oError: 0.4206  conv: 0.4085  avgPre: 0.6470\n","Batch: [0111/0300]  training loss: 0.9764  meta_loss: 1.5002  hLoss: 0.2095  rLoss: 0.2354  oError: 0.4462  conv: 0.4033  avgPre: 0.6204\n","Batch: [0112/0300]  training loss: 1.0330  meta_loss: 1.5637  hLoss: 0.2147  rLoss: 0.2448  oError: 0.4601  conv: 0.4182  avgPre: 0.6227\n","Batch: [0113/0300]  training loss: 0.9726  meta_loss: 1.5081  hLoss: 0.2123  rLoss: 0.2459  oError: 0.4755  conv: 0.4111  avgPre: 0.6141\n","Batch: [0114/0300]  training loss: 0.9905  meta_loss: 1.5859  hLoss: 0.2181  rLoss: 0.2498  oError: 0.5172  conv: 0.4143  avgPre: 0.6023\n","Batch: [0115/0300]  training loss: 0.9828  meta_loss: 1.5118  hLoss: 0.2167  rLoss: 0.2475  oError: 0.5355  conv: 0.4144  avgPre: 0.5897\n","Batch: [0116/0300]  training loss: 0.9671  meta_loss: 1.4606  hLoss: 0.2050  rLoss: 0.2410  oError: 0.4828  conv: 0.4076  avgPre: 0.6097\n","Batch: [0117/0300]  training loss: 0.9950  meta_loss: 1.6129  hLoss: 0.2146  rLoss: 0.2429  oError: 0.4755  conv: 0.4168  avgPre: 0.6160\n","Batch: [0118/0300]  training loss: 0.9905  meta_loss: 1.5021  hLoss: 0.2162  rLoss: 0.2412  oError: 0.4748  conv: 0.4033  avgPre: 0.6171\n","Batch: [0119/0300]  training loss: 0.9678  meta_loss: 1.4624  hLoss: 0.2085  rLoss: 0.2564  oError: 0.5289  conv: 0.4204  avgPre: 0.5817\n","Batch: [0120/0300]  training loss: 1.0089  meta_loss: 1.6220  hLoss: 0.2165  rLoss: 0.2393  oError: 0.4952  conv: 0.4073  avgPre: 0.6112\n","Batch: [0121/0300]  training loss: 1.0121  meta_loss: 1.5549  hLoss: 0.2149  rLoss: 0.2491  oError: 0.4667  conv: 0.4228  avgPre: 0.6115\n","Batch: [0122/0300]  training loss: 1.0090  meta_loss: 1.5443  hLoss: 0.2064  rLoss: 0.2478  oError: 0.4974  conv: 0.4221  avgPre: 0.6027\n","Batch: [0123/0300]  training loss: 1.0008  meta_loss: 1.5371  hLoss: 0.2042  rLoss: 0.2474  oError: 0.4799  conv: 0.4175  avgPre: 0.6037\n","Batch: [0124/0300]  training loss: 0.9795  meta_loss: 1.4755  hLoss: 0.2129  rLoss: 0.2438  oError: 0.4872  conv: 0.4073  avgPre: 0.6051\n","Batch: [0125/0300]  training loss: 0.9792  meta_loss: 1.4747  hLoss: 0.2197  rLoss: 0.2526  oError: 0.5472  conv: 0.4061  avgPre: 0.5872\n","Batch: [0126/0300]  training loss: 0.9820  meta_loss: 1.4892  hLoss: 0.2144  rLoss: 0.2465  oError: 0.4923  conv: 0.4162  avgPre: 0.6076\n","Batch: [0127/0300]  training loss: 1.0028  meta_loss: 1.6040  hLoss: 0.2164  rLoss: 0.2534  oError: 0.5406  conv: 0.4234  avgPre: 0.5803\n","Batch: [0128/0300]  training loss: 0.9838  meta_loss: 1.5344  hLoss: 0.2097  rLoss: 0.2382  oError: 0.4857  conv: 0.4019  avgPre: 0.6161\n","Batch: [0129/0300]  training loss: 0.9933  meta_loss: 1.5139  hLoss: 0.2161  rLoss: 0.2462  oError: 0.4974  conv: 0.4214  avgPre: 0.6067\n","Batch: [0130/0300]  training loss: 0.9766  meta_loss: 1.4685  hLoss: 0.2151  rLoss: 0.2299  oError: 0.4770  conv: 0.4024  avgPre: 0.6250\n","Batch: [0131/0300]  training loss: 0.9859  meta_loss: 1.4224  hLoss: 0.2088  rLoss: 0.2398  oError: 0.4945  conv: 0.4009  avgPre: 0.6135\n","Batch: [0132/0300]  training loss: 1.0074  meta_loss: 1.5853  hLoss: 0.2159  rLoss: 0.2556  oError: 0.4835  conv: 0.4250  avgPre: 0.6010\n","Batch: [0133/0300]  training loss: 0.9909  meta_loss: 1.5552  hLoss: 0.2153  rLoss: 0.2306  oError: 0.4572  conv: 0.3997  avgPre: 0.6278\n","Batch: [0134/0300]  training loss: 0.9789  meta_loss: 1.4417  hLoss: 0.2140  rLoss: 0.2306  oError: 0.4535  conv: 0.4026  avgPre: 0.6317\n","Batch: [0135/0300]  training loss: 0.9910  meta_loss: 1.5469  hLoss: 0.2197  rLoss: 0.2538  oError: 0.5538  conv: 0.4242  avgPre: 0.5761\n","Batch: [0136/0300]  training loss: 1.0238  meta_loss: 1.6238  hLoss: 0.2171  rLoss: 0.2559  oError: 0.5311  conv: 0.4182  avgPre: 0.5867\n","Batch: [0137/0300]  training loss: 1.0196  meta_loss: 1.6549  hLoss: 0.2157  rLoss: 0.2398  oError: 0.4418  conv: 0.4204  avgPre: 0.6322\n","Batch: [0138/0300]  training loss: 0.9975  meta_loss: 1.5570  hLoss: 0.2047  rLoss: 0.2381  oError: 0.4616  conv: 0.4097  avgPre: 0.6235\n","Batch: [0139/0300]  training loss: 0.9830  meta_loss: 1.4891  hLoss: 0.2092  rLoss: 0.2360  oError: 0.4799  conv: 0.3966  avgPre: 0.6165\n","Batch: [0140/0300]  training loss: 0.9634  meta_loss: 1.4222  hLoss: 0.2138  rLoss: 0.2298  oError: 0.4455  conv: 0.3950  avgPre: 0.6385\n","Batch: [0141/0300]  training loss: 0.9746  meta_loss: 1.5304  hLoss: 0.2157  rLoss: 0.2288  oError: 0.4294  conv: 0.3997  avgPre: 0.6394\n","Batch: [0142/0300]  training loss: 1.0099  meta_loss: 1.5493  hLoss: 0.2077  rLoss: 0.2490  oError: 0.4989  conv: 0.4164  avgPre: 0.5957\n","Batch: [0143/0300]  training loss: 0.9712  meta_loss: 1.4784  hLoss: 0.2151  rLoss: 0.2319  oError: 0.4887  conv: 0.3986  avgPre: 0.6242\n","Batch: [0144/0300]  training loss: 0.9604  meta_loss: 1.5038  hLoss: 0.2145  rLoss: 0.2314  oError: 0.4404  conv: 0.3963  avgPre: 0.6402\n","Batch: [0145/0300]  training loss: 0.9940  meta_loss: 1.5184  hLoss: 0.2163  rLoss: 0.2335  oError: 0.4945  conv: 0.3980  avgPre: 0.6168\n","Batch: [0146/0300]  training loss: 0.9640  meta_loss: 1.4449  hLoss: 0.2164  rLoss: 0.2382  oError: 0.4850  conv: 0.4014  avgPre: 0.6186\n","Batch: [0147/0300]  training loss: 0.9864  meta_loss: 1.5138  hLoss: 0.2102  rLoss: 0.2428  oError: 0.5077  conv: 0.4063  avgPre: 0.6006\n","Batch: [0148/0300]  training loss: 1.0275  meta_loss: 1.6056  hLoss: 0.2150  rLoss: 0.2330  oError: 0.4704  conv: 0.3979  avgPre: 0.6273\n","Batch: [0149/0300]  training loss: 0.9716  meta_loss: 1.4548  hLoss: 0.2092  rLoss: 0.2456  oError: 0.4762  conv: 0.4142  avgPre: 0.6075\n","Batch: [0150/0300]  training loss: 0.9879  meta_loss: 1.4965  hLoss: 0.2161  rLoss: 0.2326  oError: 0.4638  conv: 0.4082  avgPre: 0.6232\n","Batch: [0151/0300]  training loss: 0.9826  meta_loss: 1.4825  hLoss: 0.2054  rLoss: 0.2509  oError: 0.5026  conv: 0.4153  avgPre: 0.5924\n","Batch: [0152/0300]  training loss: 1.0045  meta_loss: 1.5039  hLoss: 0.2086  rLoss: 0.2459  oError: 0.5252  conv: 0.4107  avgPre: 0.6041\n","Batch: [0153/0300]  training loss: 0.9933  meta_loss: 1.5669  hLoss: 0.2151  rLoss: 0.2508  oError: 0.4499  conv: 0.4258  avgPre: 0.6129\n","Batch: [0154/0300]  training loss: 1.0039  meta_loss: 1.5879  hLoss: 0.2222  rLoss: 0.2542  oError: 0.5523  conv: 0.4236  avgPre: 0.5765\n","Batch: [0155/0300]  training loss: 1.0169  meta_loss: 1.5390  hLoss: 0.2107  rLoss: 0.2551  oError: 0.5252  conv: 0.4232  avgPre: 0.5858\n","Batch: [0156/0300]  training loss: 1.0031  meta_loss: 1.5210  hLoss: 0.2119  rLoss: 0.2399  oError: 0.5201  conv: 0.4081  avgPre: 0.6093\n","Batch: [0157/0300]  training loss: 0.9538  meta_loss: 1.3804  hLoss: 0.2152  rLoss: 0.2432  oError: 0.4879  conv: 0.4100  avgPre: 0.6099\n","Batch: [0158/0300]  training loss: 0.9505  meta_loss: 1.3822  hLoss: 0.2143  rLoss: 0.2352  oError: 0.4228  conv: 0.4007  avgPre: 0.6436\n","Batch: [0159/0300]  training loss: 0.9769  meta_loss: 1.4956  hLoss: 0.2166  rLoss: 0.2337  oError: 0.4550  conv: 0.4031  avgPre: 0.6311\n","Batch: [0160/0300]  training loss: 1.0139  meta_loss: 1.5795  hLoss: 0.2155  rLoss: 0.2403  oError: 0.5311  conv: 0.4082  avgPre: 0.6024\n","Batch: [0161/0300]  training loss: 0.9979  meta_loss: 1.5282  hLoss: 0.2126  rLoss: 0.2281  oError: 0.4075  conv: 0.4008  avgPre: 0.6482\n","Batch: [0162/0300]  training loss: 0.9709  meta_loss: 1.5065  hLoss: 0.2129  rLoss: 0.2472  oError: 0.4638  conv: 0.4202  avgPre: 0.6164\n","Batch: [0163/0300]  training loss: 0.9768  meta_loss: 1.4958  hLoss: 0.2059  rLoss: 0.2371  oError: 0.4514  conv: 0.4095  avgPre: 0.6192\n","Batch: [0164/0300]  training loss: 0.9806  meta_loss: 1.4564  hLoss: 0.2094  rLoss: 0.2368  oError: 0.4075  conv: 0.4110  avgPre: 0.6398\n","Batch: [0165/0300]  training loss: 0.9692  meta_loss: 1.4742  hLoss: 0.2109  rLoss: 0.2512  oError: 0.5318  conv: 0.4087  avgPre: 0.5947\n","Batch: [0166/0300]  training loss: 0.9948  meta_loss: 1.5526  hLoss: 0.2097  rLoss: 0.2373  oError: 0.4631  conv: 0.4022  avgPre: 0.6315\n","Batch: [0167/0300]  training loss: 0.9684  meta_loss: 1.4398  hLoss: 0.2183  rLoss: 0.2487  oError: 0.5055  conv: 0.4058  avgPre: 0.6019\n","Batch: [0168/0300]  training loss: 1.0126  meta_loss: 1.6084  hLoss: 0.2121  rLoss: 0.2467  oError: 0.4843  conv: 0.4179  avgPre: 0.6021\n","Batch: [0169/0300]  training loss: 0.9722  meta_loss: 1.4901  hLoss: 0.2077  rLoss: 0.2325  oError: 0.4579  conv: 0.4038  avgPre: 0.6312\n","Batch: [0170/0300]  training loss: 0.9906  meta_loss: 1.4838  hLoss: 0.2045  rLoss: 0.2340  oError: 0.4521  conv: 0.4033  avgPre: 0.6268\n","Batch: [0171/0300]  training loss: 1.0029  meta_loss: 1.4964  hLoss: 0.2088  rLoss: 0.2492  oError: 0.5069  conv: 0.4165  avgPre: 0.5940\n","Batch: [0172/0300]  training loss: 0.9985  meta_loss: 1.5479  hLoss: 0.2079  rLoss: 0.2340  oError: 0.4404  conv: 0.4064  avgPre: 0.6377\n","Batch: [0173/0300]  training loss: 0.9665  meta_loss: 1.4380  hLoss: 0.2142  rLoss: 0.2482  oError: 0.5157  conv: 0.4166  avgPre: 0.5945\n","Batch: [0174/0300]  training loss: 0.9860  meta_loss: 1.4882  hLoss: 0.2083  rLoss: 0.2307  oError: 0.4243  conv: 0.3966  avgPre: 0.6440\n","Batch: [0175/0300]  training loss: 0.9791  meta_loss: 1.4587  hLoss: 0.2161  rLoss: 0.2492  oError: 0.5333  conv: 0.4157  avgPre: 0.5930\n","Batch: [0176/0300]  training loss: 1.0209  meta_loss: 1.5756  hLoss: 0.1967  rLoss: 0.2562  oError: 0.4594  conv: 0.4304  avgPre: 0.6054\n","Batch: [0177/0300]  training loss: 1.0111  meta_loss: 1.6305  hLoss: 0.2115  rLoss: 0.2430  oError: 0.4638  conv: 0.4109  avgPre: 0.6172\n","Batch: [0178/0300]  training loss: 0.9768  meta_loss: 1.4689  hLoss: 0.2046  rLoss: 0.2485  oError: 0.5048  conv: 0.4248  avgPre: 0.5922\n","Batch: [0179/0300]  training loss: 0.9924  meta_loss: 1.5695  hLoss: 0.2054  rLoss: 0.2438  oError: 0.4835  conv: 0.4126  avgPre: 0.6117\n","Batch: [0180/0300]  training loss: 1.0189  meta_loss: 1.6974  hLoss: 0.2191  rLoss: 0.2537  oError: 0.5699  conv: 0.4024  avgPre: 0.5810\n","Batch: [0181/0300]  training loss: 1.0058  meta_loss: 1.5792  hLoss: 0.2085  rLoss: 0.2414  oError: 0.4696  conv: 0.4126  avgPre: 0.6108\n","Batch: [0182/0300]  training loss: 0.9879  meta_loss: 1.5696  hLoss: 0.2130  rLoss: 0.2461  oError: 0.4799  conv: 0.4129  avgPre: 0.6155\n","Batch: [0183/0300]  training loss: 0.9704  meta_loss: 1.5017  hLoss: 0.2165  rLoss: 0.2439  oError: 0.5004  conv: 0.4149  avgPre: 0.6005\n","Batch: [0184/0300]  training loss: 0.9577  meta_loss: 1.4176  hLoss: 0.2153  rLoss: 0.2339  oError: 0.5099  conv: 0.4015  avgPre: 0.6143\n","Batch: [0185/0300]  training loss: 0.9664  meta_loss: 1.4285  hLoss: 0.2167  rLoss: 0.2392  oError: 0.4777  conv: 0.4130  avgPre: 0.6087\n","Batch: [0186/0300]  training loss: 0.9700  meta_loss: 1.4149  hLoss: 0.2150  rLoss: 0.2445  oError: 0.4857  conv: 0.4121  avgPre: 0.6049\n","Batch: [0187/0300]  training loss: 0.9664  meta_loss: 1.4167  hLoss: 0.2048  rLoss: 0.2641  oError: 0.5252  conv: 0.4245  avgPre: 0.5692\n","Batch: [0188/0300]  training loss: 1.0049  meta_loss: 1.5971  hLoss: 0.2061  rLoss: 0.2384  oError: 0.4170  conv: 0.4130  avgPre: 0.6367\n","Batch: [0189/0300]  training loss: 0.9830  meta_loss: 1.5202  hLoss: 0.2173  rLoss: 0.2303  oError: 0.4572  conv: 0.3928  avgPre: 0.6332\n","Batch: [0190/0300]  training loss: 0.9652  meta_loss: 1.4592  hLoss: 0.2169  rLoss: 0.2480  oError: 0.5121  conv: 0.4093  avgPre: 0.5980\n","Batch: [0191/0300]  training loss: 0.9891  meta_loss: 1.5510  hLoss: 0.2123  rLoss: 0.2463  oError: 0.5223  conv: 0.4212  avgPre: 0.6078\n","Batch: [0192/0300]  training loss: 0.9732  meta_loss: 1.5002  hLoss: 0.2006  rLoss: 0.2362  oError: 0.4316  conv: 0.4096  avgPre: 0.6294\n","Batch: [0193/0300]  training loss: 0.9703  meta_loss: 1.4117  hLoss: 0.2169  rLoss: 0.2441  oError: 0.5062  conv: 0.4125  avgPre: 0.6021\n","Batch: [0194/0300]  training loss: 0.9690  meta_loss: 1.4310  hLoss: 0.2075  rLoss: 0.2473  oError: 0.5128  conv: 0.4111  avgPre: 0.5988\n","Batch: [0195/0300]  training loss: 0.9734  meta_loss: 1.4930  hLoss: 0.2081  rLoss: 0.2402  oError: 0.4506  conv: 0.4090  avgPre: 0.6248\n","Batch: [0196/0300]  training loss: 0.9978  meta_loss: 1.6074  hLoss: 0.2110  rLoss: 0.2346  oError: 0.4257  conv: 0.4074  avgPre: 0.6334\n","Batch: [0197/0300]  training loss: 0.9757  meta_loss: 1.5234  hLoss: 0.2097  rLoss: 0.2428  oError: 0.4587  conv: 0.4139  avgPre: 0.6137\n","Batch: [0198/0300]  training loss: 0.9913  meta_loss: 1.5984  hLoss: 0.2164  rLoss: 0.2300  oError: 0.4550  conv: 0.4016  avgPre: 0.6322\n","Batch: [0199/0300]  training loss: 0.9635  meta_loss: 1.5111  hLoss: 0.2070  rLoss: 0.2415  oError: 0.4952  conv: 0.4140  avgPre: 0.6019\n","Batch: [0200/0300]  training loss: 0.9755  meta_loss: 1.4385  hLoss: 0.2171  rLoss: 0.2489  oError: 0.5040  conv: 0.4138  avgPre: 0.6012\n","Batch: [0201/0300]  training loss: 0.9650  meta_loss: 1.4272  hLoss: 0.2123  rLoss: 0.2301  oError: 0.4272  conv: 0.4067  avgPre: 0.6393\n","Batch: [0202/0300]  training loss: 0.9538  meta_loss: 1.4254  hLoss: 0.2141  rLoss: 0.2513  oError: 0.5399  conv: 0.4220  avgPre: 0.5845\n","Batch: [0203/0300]  training loss: 0.9718  meta_loss: 1.4490  hLoss: 0.2153  rLoss: 0.2465  oError: 0.4777  conv: 0.4163  avgPre: 0.6137\n","Batch: [0204/0300]  training loss: 0.9751  meta_loss: 1.4678  hLoss: 0.2152  rLoss: 0.2257  oError: 0.4411  conv: 0.3897  avgPre: 0.6434\n","Batch: [0205/0300]  training loss: 0.9687  meta_loss: 1.4376  hLoss: 0.2166  rLoss: 0.2439  oError: 0.5091  conv: 0.4101  avgPre: 0.6014\n","Batch: [0206/0300]  training loss: 0.9725  meta_loss: 1.5026  hLoss: 0.2177  rLoss: 0.2326  oError: 0.4682  conv: 0.3946  avgPre: 0.6245\n","Batch: [0207/0300]  training loss: 0.9784  meta_loss: 1.4511  hLoss: 0.2147  rLoss: 0.2475  oError: 0.5179  conv: 0.4263  avgPre: 0.5957\n","Batch: [0208/0300]  training loss: 0.9874  meta_loss: 1.4633  hLoss: 0.2067  rLoss: 0.2299  oError: 0.4821  conv: 0.4015  avgPre: 0.6248\n","Batch: [0209/0300]  training loss: 0.9726  meta_loss: 1.5680  hLoss: 0.2145  rLoss: 0.2380  oError: 0.4572  conv: 0.4099  avgPre: 0.6315\n","Batch: [0210/0300]  training loss: 0.9804  meta_loss: 1.4513  hLoss: 0.2165  rLoss: 0.2269  oError: 0.4470  conv: 0.3909  avgPre: 0.6398\n","Batch: [0211/0300]  training loss: 0.9985  meta_loss: 1.5579  hLoss: 0.2162  rLoss: 0.2279  oError: 0.4543  conv: 0.4002  avgPre: 0.6328\n","Batch: [0212/0300]  training loss: 0.9686  meta_loss: 1.4233  hLoss: 0.2148  rLoss: 0.2470  oError: 0.4828  conv: 0.4099  avgPre: 0.6120\n","Batch: [0213/0300]  training loss: 0.9965  meta_loss: 1.5398  hLoss: 0.2041  rLoss: 0.2461  oError: 0.4770  conv: 0.4174  avgPre: 0.6054\n","Batch: [0214/0300]  training loss: 1.0066  meta_loss: 1.6439  hLoss: 0.2169  rLoss: 0.2469  oError: 0.5121  conv: 0.4048  avgPre: 0.6010\n","Batch: [0215/0300]  training loss: 0.9890  meta_loss: 1.5757  hLoss: 0.2080  rLoss: 0.2553  oError: 0.4996  conv: 0.4147  avgPre: 0.5911\n","Batch: [0216/0300]  training loss: 0.9952  meta_loss: 1.6149  hLoss: 0.2058  rLoss: 0.2548  oError: 0.4967  conv: 0.4269  avgPre: 0.5846\n","Batch: [0217/0300]  training loss: 1.0079  meta_loss: 1.5866  hLoss: 0.2036  rLoss: 0.2313  oError: 0.4609  conv: 0.3916  avgPre: 0.6327\n","Batch: [0218/0300]  training loss: 0.9691  meta_loss: 1.4305  hLoss: 0.2159  rLoss: 0.2303  oError: 0.4865  conv: 0.3932  avgPre: 0.6230\n","Batch: [0219/0300]  training loss: 0.9604  meta_loss: 1.4454  hLoss: 0.2163  rLoss: 0.2343  oError: 0.4638  conv: 0.3960  avgPre: 0.6251\n","Batch: [0220/0300]  training loss: 0.9905  meta_loss: 1.5643  hLoss: 0.2117  rLoss: 0.2409  oError: 0.4660  conv: 0.4091  avgPre: 0.6102\n","Batch: [0221/0300]  training loss: 1.0266  meta_loss: 1.6719  hLoss: 0.2141  rLoss: 0.2333  oError: 0.4521  conv: 0.3973  avgPre: 0.6364\n","Batch: [0222/0300]  training loss: 0.9928  meta_loss: 1.5366  hLoss: 0.2077  rLoss: 0.2328  oError: 0.4850  conv: 0.3976  avgPre: 0.6246\n","Batch: [0223/0300]  training loss: 0.9792  meta_loss: 1.4981  hLoss: 0.2134  rLoss: 0.2340  oError: 0.4499  conv: 0.4017  avgPre: 0.6330\n","Batch: [0224/0300]  training loss: 0.9502  meta_loss: 1.4147  hLoss: 0.2164  rLoss: 0.2411  oError: 0.5106  conv: 0.4084  avgPre: 0.6094\n","Batch: [0225/0300]  training loss: 0.9591  meta_loss: 1.4444  hLoss: 0.2171  rLoss: 0.2428  oError: 0.4872  conv: 0.4117  avgPre: 0.6071\n","Batch: [0226/0300]  training loss: 0.9801  meta_loss: 1.4673  hLoss: 0.2144  rLoss: 0.2347  oError: 0.4323  conv: 0.4216  avgPre: 0.6372\n","Batch: [0227/0300]  training loss: 0.9743  meta_loss: 1.5040  hLoss: 0.2057  rLoss: 0.2350  oError: 0.4543  conv: 0.4087  avgPre: 0.6244\n","Batch: [0228/0300]  training loss: 0.9743  meta_loss: 1.5286  hLoss: 0.2145  rLoss: 0.2358  oError: 0.4616  conv: 0.4093  avgPre: 0.6201\n","Batch: [0229/0300]  training loss: 0.9608  meta_loss: 1.5207  hLoss: 0.2155  rLoss: 0.2373  oError: 0.4718  conv: 0.3995  avgPre: 0.6230\n","Batch: [0230/0300]  training loss: 0.9934  meta_loss: 1.5150  hLoss: 0.2108  rLoss: 0.2497  oError: 0.4850  conv: 0.4210  avgPre: 0.5998\n","Batch: [0231/0300]  training loss: 0.9887  meta_loss: 1.5267  hLoss: 0.2141  rLoss: 0.2378  oError: 0.4433  conv: 0.4105  avgPre: 0.6204\n","Batch: [0232/0300]  training loss: 0.9864  meta_loss: 1.5624  hLoss: 0.2163  rLoss: 0.2341  oError: 0.4733  conv: 0.4011  avgPre: 0.6188\n","Batch: [0233/0300]  training loss: 0.9718  meta_loss: 1.4962  hLoss: 0.2166  rLoss: 0.2345  oError: 0.4916  conv: 0.3974  avgPre: 0.6166\n","Batch: [0234/0300]  training loss: 0.9656  meta_loss: 1.4056  hLoss: 0.2169  rLoss: 0.2435  oError: 0.5552  conv: 0.4109  avgPre: 0.5955\n","Batch: [0235/0300]  training loss: 0.9693  meta_loss: 1.5120  hLoss: 0.2148  rLoss: 0.2338  oError: 0.4601  conv: 0.4041  avgPre: 0.6342\n","Batch: [0236/0300]  training loss: 1.0042  meta_loss: 1.6495  hLoss: 0.2061  rLoss: 0.2557  oError: 0.5099  conv: 0.4282  avgPre: 0.5852\n","Batch: [0237/0300]  training loss: 0.9787  meta_loss: 1.5543  hLoss: 0.2174  rLoss: 0.2419  oError: 0.4850  conv: 0.4123  avgPre: 0.6153\n","Batch: [0238/0300]  training loss: 0.9647  meta_loss: 1.4713  hLoss: 0.2090  rLoss: 0.2513  oError: 0.4755  conv: 0.4345  avgPre: 0.6000\n","Batch: [0239/0300]  training loss: 1.0139  meta_loss: 1.6281  hLoss: 0.2053  rLoss: 0.2347  oError: 0.4367  conv: 0.4010  avgPre: 0.6293\n","Batch: [0240/0300]  training loss: 0.9978  meta_loss: 1.5394  hLoss: 0.2147  rLoss: 0.2337  oError: 0.4477  conv: 0.4021  avgPre: 0.6230\n","Batch: [0241/0300]  training loss: 0.9848  meta_loss: 1.5018  hLoss: 0.2060  rLoss: 0.2403  oError: 0.5011  conv: 0.4097  avgPre: 0.6066\n","Batch: [0242/0300]  training loss: 1.0197  meta_loss: 1.5526  hLoss: 0.2001  rLoss: 0.2529  oError: 0.4894  conv: 0.4283  avgPre: 0.5983\n","Batch: [0243/0300]  training loss: 1.0060  meta_loss: 1.6008  hLoss: 0.2020  rLoss: 0.2352  oError: 0.4631  conv: 0.3986  avgPre: 0.6229\n","Batch: [0244/0300]  training loss: 1.0093  meta_loss: 1.6011  hLoss: 0.2127  rLoss: 0.2458  oError: 0.4682  conv: 0.4092  avgPre: 0.6142\n","Batch: [0245/0300]  training loss: 1.0020  meta_loss: 1.4984  hLoss: 0.2101  rLoss: 0.2558  oError: 0.5150  conv: 0.4248  avgPre: 0.5880\n","Batch: [0246/0300]  training loss: 0.9688  meta_loss: 1.5328  hLoss: 0.2140  rLoss: 0.2479  oError: 0.5304  conv: 0.4123  avgPre: 0.5903\n","Batch: [0247/0300]  training loss: 1.0015  meta_loss: 1.5773  hLoss: 0.2064  rLoss: 0.2363  oError: 0.4404  conv: 0.4035  avgPre: 0.6284\n","Batch: [0248/0300]  training loss: 0.9677  meta_loss: 1.5018  hLoss: 0.2153  rLoss: 0.2322  oError: 0.4514  conv: 0.3995  avgPre: 0.6237\n","Batch: [0249/0300]  training loss: 0.9724  meta_loss: 1.5932  hLoss: 0.2134  rLoss: 0.2444  oError: 0.5135  conv: 0.4207  avgPre: 0.6021\n","Batch: [0250/0300]  training loss: 0.9729  meta_loss: 1.5438  hLoss: 0.2139  rLoss: 0.2422  oError: 0.4850  conv: 0.4081  avgPre: 0.6077\n","Batch: [0251/0300]  training loss: 0.9683  meta_loss: 1.5792  hLoss: 0.2167  rLoss: 0.2563  oError: 0.5355  conv: 0.4326  avgPre: 0.5779\n","Batch: [0252/0300]  training loss: 0.9576  meta_loss: 1.4756  hLoss: 0.2141  rLoss: 0.2275  oError: 0.4140  conv: 0.3997  avgPre: 0.6446\n","Batch: [0253/0300]  training loss: 0.9504  meta_loss: 1.4219  hLoss: 0.2143  rLoss: 0.2334  oError: 0.4206  conv: 0.4103  avgPre: 0.6377\n","Batch: [0254/0300]  training loss: 0.9531  meta_loss: 1.4666  hLoss: 0.2129  rLoss: 0.2356  oError: 0.4718  conv: 0.4046  avgPre: 0.6231\n","Batch: [0255/0300]  training loss: 0.9582  meta_loss: 1.4810  hLoss: 0.2142  rLoss: 0.2322  oError: 0.4433  conv: 0.4072  avgPre: 0.6335\n","Batch: [0256/0300]  training loss: 0.9506  meta_loss: 1.4564  hLoss: 0.2143  rLoss: 0.2518  oError: 0.4879  conv: 0.4299  avgPre: 0.6025\n","Batch: [0257/0300]  training loss: 0.9917  meta_loss: 1.5441  hLoss: 0.2157  rLoss: 0.2695  oError: 0.5033  conv: 0.4308  avgPre: 0.5732\n","Batch: [0258/0300]  training loss: 1.0090  meta_loss: 1.6853  hLoss: 0.2038  rLoss: 0.2329  oError: 0.4309  conv: 0.4031  avgPre: 0.6354\n","Batch: [0259/0300]  training loss: 0.9750  meta_loss: 1.5092  hLoss: 0.2130  rLoss: 0.2441  oError: 0.5194  conv: 0.4150  avgPre: 0.5992\n","Batch: [0260/0300]  training loss: 0.9470  meta_loss: 1.4624  hLoss: 0.2155  rLoss: 0.2235  oError: 0.4294  conv: 0.3972  avgPre: 0.6461\n","Batch: [0261/0300]  training loss: 0.9665  meta_loss: 1.4541  hLoss: 0.2094  rLoss: 0.2473  oError: 0.5128  conv: 0.4132  avgPre: 0.6062\n","Batch: [0262/0300]  training loss: 0.9991  meta_loss: 1.5936  hLoss: 0.2122  rLoss: 0.2433  oError: 0.4528  conv: 0.4264  avgPre: 0.6142\n","Batch: [0263/0300]  training loss: 0.9648  meta_loss: 1.4279  hLoss: 0.2165  rLoss: 0.2393  oError: 0.4755  conv: 0.4122  avgPre: 0.6094\n","Batch: [0264/0300]  training loss: 0.9528  meta_loss: 1.4708  hLoss: 0.2167  rLoss: 0.2321  oError: 0.4572  conv: 0.4061  avgPre: 0.6293\n","Batch: [0265/0300]  training loss: 0.9674  meta_loss: 1.4623  hLoss: 0.2166  rLoss: 0.2428  oError: 0.5011  conv: 0.4145  avgPre: 0.5970\n","Batch: [0266/0300]  training loss: 0.9864  meta_loss: 1.5630  hLoss: 0.2144  rLoss: 0.2300  oError: 0.4579  conv: 0.3955  avgPre: 0.6306\n","Batch: [0267/0300]  training loss: 0.9436  meta_loss: 1.4279  hLoss: 0.2113  rLoss: 0.2474  oError: 0.4960  conv: 0.4194  avgPre: 0.6040\n","Batch: [0268/0300]  training loss: 0.9463  meta_loss: 1.3971  hLoss: 0.2086  rLoss: 0.2424  oError: 0.4535  conv: 0.4143  avgPre: 0.6213\n","Batch: [0269/0300]  training loss: 0.9598  meta_loss: 1.4926  hLoss: 0.2134  rLoss: 0.2384  oError: 0.4440  conv: 0.4080  avgPre: 0.6251\n","Batch: [0270/0300]  training loss: 0.9460  meta_loss: 1.3778  hLoss: 0.2154  rLoss: 0.2287  oError: 0.4609  conv: 0.3975  avgPre: 0.6322\n","Batch: [0271/0300]  training loss: 0.9736  meta_loss: 1.5232  hLoss: 0.2137  rLoss: 0.2331  oError: 0.4338  conv: 0.4043  avgPre: 0.6358\n","Batch: [0272/0300]  training loss: 0.9667  meta_loss: 1.5017  hLoss: 0.2141  rLoss: 0.2395  oError: 0.4528  conv: 0.4128  avgPre: 0.6307\n","Batch: [0273/0300]  training loss: 0.9809  meta_loss: 1.5218  hLoss: 0.2099  rLoss: 0.2193  oError: 0.4396  conv: 0.3858  avgPre: 0.6494\n","Batch: [0274/0300]  training loss: 0.9575  meta_loss: 1.4423  hLoss: 0.2069  rLoss: 0.2352  oError: 0.4784  conv: 0.4072  avgPre: 0.6265\n","Batch: [0275/0300]  training loss: 0.9779  meta_loss: 1.5125  hLoss: 0.2152  rLoss: 0.2507  oError: 0.5091  conv: 0.4319  avgPre: 0.6003\n","Batch: [0276/0300]  training loss: 0.9698  meta_loss: 1.4895  hLoss: 0.2146  rLoss: 0.2447  oError: 0.4894  conv: 0.4152  avgPre: 0.6042\n","Batch: [0277/0300]  training loss: 0.9799  meta_loss: 1.4727  hLoss: 0.2169  rLoss: 0.2551  oError: 0.5472  conv: 0.4249  avgPre: 0.5844\n","Batch: [0278/0300]  training loss: 0.9672  meta_loss: 1.3888  hLoss: 0.2127  rLoss: 0.2441  oError: 0.4696  conv: 0.4163  avgPre: 0.6208\n","Batch: [0279/0300]  training loss: 1.0012  meta_loss: 1.6815  hLoss: 0.2145  rLoss: 0.2232  oError: 0.4279  conv: 0.3929  avgPre: 0.6462\n","Batch: [0280/0300]  training loss: 0.9583  meta_loss: 1.4226  hLoss: 0.2073  rLoss: 0.2392  oError: 0.4653  conv: 0.4059  avgPre: 0.6166\n","Batch: [0281/0300]  training loss: 0.9764  meta_loss: 1.5469  hLoss: 0.2101  rLoss: 0.2454  oError: 0.5004  conv: 0.4126  avgPre: 0.6015\n","Batch: [0282/0300]  training loss: 0.9683  meta_loss: 1.4982  hLoss: 0.2167  rLoss: 0.2444  oError: 0.4587  conv: 0.4281  avgPre: 0.6161\n","Batch: [0283/0300]  training loss: 0.9746  meta_loss: 1.4759  hLoss: 0.2145  rLoss: 0.2450  oError: 0.5377  conv: 0.4105  avgPre: 0.5964\n","Batch: [0284/0300]  training loss: 0.9751  meta_loss: 1.5101  hLoss: 0.2266  rLoss: 0.2718  oError: 0.6116  conv: 0.4368  avgPre: 0.5417\n","Batch: [0285/0300]  training loss: 0.9680  meta_loss: 1.4450  hLoss: 0.2095  rLoss: 0.2337  oError: 0.4674  conv: 0.4037  avgPre: 0.6278\n","Batch: [0286/0300]  training loss: 0.9693  meta_loss: 1.4762  hLoss: 0.2018  rLoss: 0.2357  oError: 0.4477  conv: 0.4005  avgPre: 0.6241\n","Batch: [0287/0300]  training loss: 0.9713  meta_loss: 1.4790  hLoss: 0.2082  rLoss: 0.2360  oError: 0.4484  conv: 0.4111  avgPre: 0.6188\n","Batch: [0288/0300]  training loss: 1.0022  meta_loss: 1.5065  hLoss: 0.2055  rLoss: 0.2627  oError: 0.4850  conv: 0.4391  avgPre: 0.5960\n","Batch: [0289/0300]  training loss: 1.0003  meta_loss: 1.4992  hLoss: 0.2091  rLoss: 0.2298  oError: 0.4294  conv: 0.4030  avgPre: 0.6358\n","Batch: [0290/0300]  training loss: 1.0042  meta_loss: 1.5800  hLoss: 0.2149  rLoss: 0.2354  oError: 0.5077  conv: 0.3983  avgPre: 0.6141\n","Batch: [0291/0300]  training loss: 0.9786  meta_loss: 1.4545  hLoss: 0.2133  rLoss: 0.2308  oError: 0.4411  conv: 0.4086  avgPre: 0.6310\n","Batch: [0292/0300]  training loss: 0.9714  meta_loss: 1.4507  hLoss: 0.2147  rLoss: 0.2483  oError: 0.5062  conv: 0.4204  avgPre: 0.5963\n","Batch: [0293/0300]  training loss: 0.9881  meta_loss: 1.5148  hLoss: 0.2156  rLoss: 0.2439  oError: 0.4887  conv: 0.4055  avgPre: 0.6141\n","Batch: [0294/0300]  training loss: 0.9811  meta_loss: 1.5416  hLoss: 0.2169  rLoss: 0.2461  oError: 0.4901  conv: 0.4144  avgPre: 0.6087\n","Batch: [0295/0300]  training loss: 0.9562  meta_loss: 1.4634  hLoss: 0.2139  rLoss: 0.2216  oError: 0.4382  conv: 0.3904  avgPre: 0.6471\n","Batch: [0296/0300]  training loss: 0.9500  meta_loss: 1.3725  hLoss: 0.2098  rLoss: 0.2331  oError: 0.4770  conv: 0.3958  avgPre: 0.6262\n","Batch: [0297/0300]  training loss: 0.9584  meta_loss: 1.4257  hLoss: 0.2134  rLoss: 0.2466  oError: 0.4572  conv: 0.4168  avgPre: 0.6116\n","Batch: [0298/0300]  training loss: 0.9611  meta_loss: 1.4388  hLoss: 0.2140  rLoss: 0.2576  oError: 0.5113  conv: 0.4270  avgPre: 0.5795\n","Batch: [0299/0300]  training loss: 0.9720  meta_loss: 1.5346  hLoss: 0.2133  rLoss: 0.2414  oError: 0.4550  conv: 0.4157  avgPre: 0.6271\n","Batch: [0300/0300]  training loss: 0.9620  meta_loss: 1.4735  hLoss: 0.2137  rLoss: 0.2486  oError: 0.4952  conv: 0.4152  avgPre: 0.6038\n","\n","Test results of the last :\t Best hLoss: 0.2137  Best rLoss: 0.2486  Best oError: 0.4952  Best conv: 0.4152  Best avgPre: 0.6038 \n","0\n","data=music_emotion/cv=0/p_noise=0/p_true=9/method=meta\n","\n","W:(4974, 492)\n","train:(4974, 98)\n","meta:(492, 98)\n","Batch: [0001/0300]  training loss: 1.6098  meta_loss: 2.1960  hLoss: 0.2119  rLoss: 0.3406  oError: 0.5969  conv: 0.4852  avgPre: 0.5217\n","Batch: [0002/0300]  training loss: 1.3223  meta_loss: 1.7265  hLoss: 0.2117  rLoss: 0.2789  oError: 0.5428  conv: 0.4441  avgPre: 0.5704\n","Batch: [0003/0300]  training loss: 1.2898  meta_loss: 1.8419  hLoss: 0.2004  rLoss: 0.2775  oError: 0.5026  conv: 0.4454  avgPre: 0.5797\n","Batch: [0004/0300]  training loss: 1.2468  meta_loss: 1.6667  hLoss: 0.2107  rLoss: 0.2886  oError: 0.5750  conv: 0.4571  avgPre: 0.5487\n","Batch: [0005/0300]  training loss: 1.2548  meta_loss: 1.7608  hLoss: 0.2072  rLoss: 0.2728  oError: 0.4821  conv: 0.4392  avgPre: 0.6001\n","Batch: [0006/0300]  training loss: 1.2749  meta_loss: 1.8064  hLoss: 0.2165  rLoss: 0.3038  oError: 0.5589  conv: 0.4672  avgPre: 0.5342\n","Batch: [0007/0300]  training loss: 1.2791  meta_loss: 1.8190  hLoss: 0.2062  rLoss: 0.2863  oError: 0.5640  conv: 0.4443  avgPre: 0.5678\n","Batch: [0008/0300]  training loss: 1.2397  meta_loss: 1.8088  hLoss: 0.2201  rLoss: 0.2602  oError: 0.5618  conv: 0.4217  avgPre: 0.5747\n","Batch: [0009/0300]  training loss: 1.2303  meta_loss: 1.6842  hLoss: 0.2192  rLoss: 0.2982  oError: 0.5984  conv: 0.4657  avgPre: 0.5372\n","Batch: [0010/0300]  training loss: 1.2125  meta_loss: 1.6786  hLoss: 0.2064  rLoss: 0.2793  oError: 0.5457  conv: 0.4441  avgPre: 0.5576\n","Batch: [0011/0300]  training loss: 1.1782  meta_loss: 1.6261  hLoss: 0.2165  rLoss: 0.2680  oError: 0.5538  conv: 0.4313  avgPre: 0.5852\n","Batch: [0012/0300]  training loss: 1.1871  meta_loss: 1.6147  hLoss: 0.2038  rLoss: 0.2724  oError: 0.4682  conv: 0.4387  avgPre: 0.5926\n","Batch: [0013/0300]  training loss: 1.1847  meta_loss: 1.6418  hLoss: 0.2178  rLoss: 0.2752  oError: 0.5135  conv: 0.4568  avgPre: 0.5621\n","Batch: [0014/0300]  training loss: 1.2261  meta_loss: 1.7536  hLoss: 0.2027  rLoss: 0.2574  oError: 0.4609  conv: 0.4299  avgPre: 0.5996\n","Batch: [0015/0300]  training loss: 1.1625  meta_loss: 1.6027  hLoss: 0.2091  rLoss: 0.2623  oError: 0.4813  conv: 0.4280  avgPre: 0.5935\n","Batch: [0016/0300]  training loss: 1.1799  meta_loss: 1.6192  hLoss: 0.2161  rLoss: 0.2743  oError: 0.5757  conv: 0.4370  avgPre: 0.5704\n","Batch: [0017/0300]  training loss: 1.1412  meta_loss: 1.4957  hLoss: 0.2099  rLoss: 0.2626  oError: 0.5391  conv: 0.4269  avgPre: 0.5889\n","Batch: [0018/0300]  training loss: 1.1351  meta_loss: 1.5468  hLoss: 0.2035  rLoss: 0.2501  oError: 0.4806  conv: 0.4119  avgPre: 0.6060\n","Batch: [0019/0300]  training loss: 1.1673  meta_loss: 1.6145  hLoss: 0.2197  rLoss: 0.2833  oError: 0.5567  conv: 0.4511  avgPre: 0.5555\n","Batch: [0020/0300]  training loss: 1.1815  meta_loss: 1.6920  hLoss: 0.2113  rLoss: 0.2682  oError: 0.5004  conv: 0.4389  avgPre: 0.5685\n","Batch: [0021/0300]  training loss: 1.1840  meta_loss: 1.7158  hLoss: 0.2141  rLoss: 0.2461  oError: 0.4982  conv: 0.4112  avgPre: 0.6160\n","Batch: [0022/0300]  training loss: 1.1589  meta_loss: 1.6925  hLoss: 0.2258  rLoss: 0.2804  oError: 0.5794  conv: 0.4517  avgPre: 0.5399\n","Batch: [0023/0300]  training loss: 1.1597  meta_loss: 1.6865  hLoss: 0.2175  rLoss: 0.2499  oError: 0.5267  conv: 0.4102  avgPre: 0.5907\n","Batch: [0024/0300]  training loss: 1.1375  meta_loss: 1.5373  hLoss: 0.2041  rLoss: 0.2533  oError: 0.4894  conv: 0.4189  avgPre: 0.5987\n","Batch: [0025/0300]  training loss: 1.1830  meta_loss: 1.7556  hLoss: 0.2101  rLoss: 0.2777  oError: 0.5413  conv: 0.4438  avgPre: 0.5605\n","Batch: [0026/0300]  training loss: 1.2197  meta_loss: 1.7745  hLoss: 0.2393  rLoss: 0.2886  oError: 0.5538  conv: 0.4444  avgPre: 0.5338\n","Batch: [0027/0300]  training loss: 1.1571  meta_loss: 1.7311  hLoss: 0.2058  rLoss: 0.2565  oError: 0.4638  conv: 0.4200  avgPre: 0.6100\n","Batch: [0028/0300]  training loss: 1.1399  meta_loss: 1.6451  hLoss: 0.2114  rLoss: 0.2504  oError: 0.4938  conv: 0.4119  avgPre: 0.5980\n","Batch: [0029/0300]  training loss: 1.1537  meta_loss: 1.6466  hLoss: 0.2238  rLoss: 0.2718  oError: 0.6020  conv: 0.4353  avgPre: 0.5442\n","Batch: [0030/0300]  training loss: 1.1519  meta_loss: 1.7047  hLoss: 0.2105  rLoss: 0.2558  oError: 0.5282  conv: 0.4184  avgPre: 0.6031\n","Batch: [0031/0300]  training loss: 1.1367  meta_loss: 1.5806  hLoss: 0.2138  rLoss: 0.2781  oError: 0.5201  conv: 0.4357  avgPre: 0.5577\n","Batch: [0032/0300]  training loss: 1.1236  meta_loss: 1.5858  hLoss: 0.2064  rLoss: 0.2453  oError: 0.5011  conv: 0.4083  avgPre: 0.6047\n","Batch: [0033/0300]  training loss: 1.1265  meta_loss: 1.5827  hLoss: 0.1961  rLoss: 0.2486  oError: 0.4506  conv: 0.4099  avgPre: 0.6174\n","Batch: [0034/0300]  training loss: 1.1368  meta_loss: 1.6332  hLoss: 0.2125  rLoss: 0.2668  oError: 0.5560  conv: 0.4291  avgPre: 0.5838\n","Batch: [0035/0300]  training loss: 1.1470  meta_loss: 1.7232  hLoss: 0.2074  rLoss: 0.2601  oError: 0.4945  conv: 0.4243  avgPre: 0.5911\n","Batch: [0036/0300]  training loss: 1.1252  meta_loss: 1.5926  hLoss: 0.2151  rLoss: 0.2616  oError: 0.5062  conv: 0.4392  avgPre: 0.5825\n","Batch: [0037/0300]  training loss: 1.1327  meta_loss: 1.6173  hLoss: 0.2046  rLoss: 0.2497  oError: 0.4777  conv: 0.4188  avgPre: 0.5965\n","Batch: [0038/0300]  training loss: 1.1412  meta_loss: 1.6171  hLoss: 0.2105  rLoss: 0.2429  oError: 0.5033  conv: 0.4065  avgPre: 0.6111\n","Batch: [0039/0300]  training loss: 1.1103  meta_loss: 1.5554  hLoss: 0.2153  rLoss: 0.2566  oError: 0.5194  conv: 0.4190  avgPre: 0.5770\n","Batch: [0040/0300]  training loss: 1.1210  meta_loss: 1.5732  hLoss: 0.2098  rLoss: 0.2323  oError: 0.4718  conv: 0.3967  avgPre: 0.6301\n","Batch: [0041/0300]  training loss: 1.0961  meta_loss: 1.5067  hLoss: 0.2066  rLoss: 0.2506  oError: 0.4909  conv: 0.4164  avgPre: 0.5949\n","Batch: [0042/0300]  training loss: 1.1541  meta_loss: 1.6391  hLoss: 0.2036  rLoss: 0.2481  oError: 0.5099  conv: 0.4134  avgPre: 0.5899\n","Batch: [0043/0300]  training loss: 1.1337  meta_loss: 1.6015  hLoss: 0.2233  rLoss: 0.2500  oError: 0.5479  conv: 0.4170  avgPre: 0.5818\n","Batch: [0044/0300]  training loss: 1.1180  meta_loss: 1.5124  hLoss: 0.2109  rLoss: 0.2576  oError: 0.5150  conv: 0.4138  avgPre: 0.5954\n","Batch: [0045/0300]  training loss: 1.1419  meta_loss: 1.6157  hLoss: 0.2062  rLoss: 0.2584  oError: 0.4770  conv: 0.4278  avgPre: 0.5954\n","Batch: [0046/0300]  training loss: 1.1427  meta_loss: 1.5844  hLoss: 0.1971  rLoss: 0.2487  oError: 0.4557  conv: 0.4095  avgPre: 0.6165\n","Batch: [0047/0300]  training loss: 1.1912  meta_loss: 1.7723  hLoss: 0.2070  rLoss: 0.2484  oError: 0.4865  conv: 0.4206  avgPre: 0.5991\n","Batch: [0048/0300]  training loss: 1.1502  meta_loss: 1.6572  hLoss: 0.2065  rLoss: 0.2655  oError: 0.5026  conv: 0.4276  avgPre: 0.5845\n","Batch: [0049/0300]  training loss: 1.1239  meta_loss: 1.6412  hLoss: 0.2080  rLoss: 0.2462  oError: 0.4806  conv: 0.4077  avgPre: 0.6107\n","Batch: [0050/0300]  training loss: 1.1299  meta_loss: 1.6277  hLoss: 0.2238  rLoss: 0.2804  oError: 0.5845  conv: 0.4241  avgPre: 0.5556\n","Batch: [0051/0300]  training loss: 1.1096  meta_loss: 1.5717  hLoss: 0.2049  rLoss: 0.2447  oError: 0.4887  conv: 0.4105  avgPre: 0.6010\n","Batch: [0052/0300]  training loss: 1.1647  meta_loss: 1.7218  hLoss: 0.2079  rLoss: 0.2487  oError: 0.4931  conv: 0.4109  avgPre: 0.5940\n","Batch: [0053/0300]  training loss: 1.1071  meta_loss: 1.5287  hLoss: 0.2015  rLoss: 0.2366  oError: 0.4440  conv: 0.4029  avgPre: 0.6254\n","Batch: [0054/0300]  training loss: 1.0989  meta_loss: 1.5058  hLoss: 0.2037  rLoss: 0.2417  oError: 0.4931  conv: 0.4069  avgPre: 0.6084\n","Batch: [0055/0300]  training loss: 1.1346  meta_loss: 1.5573  hLoss: 0.2108  rLoss: 0.2687  oError: 0.5208  conv: 0.4244  avgPre: 0.5773\n","Batch: [0056/0300]  training loss: 1.1085  meta_loss: 1.5482  hLoss: 0.2042  rLoss: 0.2488  oError: 0.4974  conv: 0.4158  avgPre: 0.6009\n","Batch: [0057/0300]  training loss: 1.1256  meta_loss: 1.6053  hLoss: 0.2042  rLoss: 0.2593  oError: 0.4960  conv: 0.4207  avgPre: 0.5921\n","Batch: [0058/0300]  training loss: 1.1175  meta_loss: 1.5540  hLoss: 0.2105  rLoss: 0.2481  oError: 0.4770  conv: 0.4130  avgPre: 0.6106\n","Batch: [0059/0300]  training loss: 1.1004  meta_loss: 1.5440  hLoss: 0.2038  rLoss: 0.2433  oError: 0.4389  conv: 0.4095  avgPre: 0.6270\n","Batch: [0060/0300]  training loss: 1.1204  meta_loss: 1.5705  hLoss: 0.2128  rLoss: 0.2390  oError: 0.4623  conv: 0.4039  avgPre: 0.6242\n","Batch: [0061/0300]  training loss: 1.1400  meta_loss: 1.6465  hLoss: 0.1947  rLoss: 0.2425  oError: 0.4557  conv: 0.4067  avgPre: 0.6206\n","Batch: [0062/0300]  training loss: 1.1229  meta_loss: 1.5627  hLoss: 0.1962  rLoss: 0.2515  oError: 0.4660  conv: 0.4210  avgPre: 0.6097\n","Batch: [0063/0300]  training loss: 1.1161  meta_loss: 1.6344  hLoss: 0.1945  rLoss: 0.2507  oError: 0.4331  conv: 0.4152  avgPre: 0.6213\n","Batch: [0064/0300]  training loss: 1.1125  meta_loss: 1.6046  hLoss: 0.2096  rLoss: 0.2601  oError: 0.5128  conv: 0.4184  avgPre: 0.5853\n","Batch: [0065/0300]  training loss: 1.1562  meta_loss: 1.7024  hLoss: 0.2016  rLoss: 0.2476  oError: 0.4704  conv: 0.4267  avgPre: 0.6099\n","Batch: [0066/0300]  training loss: 1.1401  meta_loss: 1.6318  hLoss: 0.2073  rLoss: 0.2409  oError: 0.4938  conv: 0.4019  avgPre: 0.6020\n","Batch: [0067/0300]  training loss: 1.1069  meta_loss: 1.5954  hLoss: 0.1992  rLoss: 0.2384  oError: 0.4587  conv: 0.4055  avgPre: 0.6158\n","Batch: [0068/0300]  training loss: 1.0935  meta_loss: 1.5645  hLoss: 0.2029  rLoss: 0.2429  oError: 0.4418  conv: 0.4238  avgPre: 0.6209\n","Batch: [0069/0300]  training loss: 1.1062  meta_loss: 1.5508  hLoss: 0.1996  rLoss: 0.2464  oError: 0.4214  conv: 0.4196  avgPre: 0.6249\n","Batch: [0070/0300]  training loss: 1.1258  meta_loss: 1.6738  hLoss: 0.2094  rLoss: 0.2540  oError: 0.5318  conv: 0.4205  avgPre: 0.5797\n","Batch: [0071/0300]  training loss: 1.0999  meta_loss: 1.5717  hLoss: 0.2161  rLoss: 0.2398  oError: 0.4967  conv: 0.4033  avgPre: 0.6128\n","Batch: [0072/0300]  training loss: 1.1438  meta_loss: 1.6181  hLoss: 0.1958  rLoss: 0.2359  oError: 0.4462  conv: 0.4042  avgPre: 0.6211\n","Batch: [0073/0300]  training loss: 1.0895  meta_loss: 1.5047  hLoss: 0.2054  rLoss: 0.2390  oError: 0.4879  conv: 0.4064  avgPre: 0.6093\n","Batch: [0074/0300]  training loss: 1.1071  meta_loss: 1.5956  hLoss: 0.2036  rLoss: 0.2520  oError: 0.4572  conv: 0.4306  avgPre: 0.5996\n","Batch: [0075/0300]  training loss: 1.1184  meta_loss: 1.5914  hLoss: 0.1971  rLoss: 0.2456  oError: 0.4565  conv: 0.4168  avgPre: 0.6066\n","Batch: [0076/0300]  training loss: 1.1435  meta_loss: 1.6453  hLoss: 0.2046  rLoss: 0.2544  oError: 0.4872  conv: 0.4226  avgPre: 0.5867\n","Batch: [0077/0300]  training loss: 1.1056  meta_loss: 1.5485  hLoss: 0.1974  rLoss: 0.2415  oError: 0.4638  conv: 0.4098  avgPre: 0.6116\n","Batch: [0078/0300]  training loss: 1.0949  meta_loss: 1.5763  hLoss: 0.2044  rLoss: 0.2436  oError: 0.4835  conv: 0.4069  avgPre: 0.6071\n","Batch: [0079/0300]  training loss: 1.1010  meta_loss: 1.5761  hLoss: 0.2156  rLoss: 0.2703  oError: 0.5311  conv: 0.4295  avgPre: 0.5782\n","Batch: [0080/0300]  training loss: 1.0985  meta_loss: 1.5597  hLoss: 0.2129  rLoss: 0.2401  oError: 0.5026  conv: 0.4071  avgPre: 0.6141\n","Batch: [0081/0300]  training loss: 1.1010  meta_loss: 1.6092  hLoss: 0.1986  rLoss: 0.2344  oError: 0.4250  conv: 0.4145  avgPre: 0.6351\n","Batch: [0082/0300]  training loss: 1.1002  meta_loss: 1.5944  hLoss: 0.2128  rLoss: 0.2456  oError: 0.4660  conv: 0.4148  avgPre: 0.6118\n","Batch: [0083/0300]  training loss: 1.1320  meta_loss: 1.6069  hLoss: 0.2032  rLoss: 0.2348  oError: 0.4492  conv: 0.3972  avgPre: 0.6367\n","Batch: [0084/0300]  training loss: 1.1244  meta_loss: 1.6001  hLoss: 0.2070  rLoss: 0.2558  oError: 0.4792  conv: 0.4317  avgPre: 0.5863\n","Batch: [0085/0300]  training loss: 1.1289  meta_loss: 1.5545  hLoss: 0.1980  rLoss: 0.2429  oError: 0.4623  conv: 0.4065  avgPre: 0.6125\n","Batch: [0086/0300]  training loss: 1.1086  meta_loss: 1.5911  hLoss: 0.1987  rLoss: 0.2463  oError: 0.4733  conv: 0.4219  avgPre: 0.6171\n","Batch: [0087/0300]  training loss: 1.1648  meta_loss: 1.7259  hLoss: 0.2198  rLoss: 0.2549  oError: 0.5808  conv: 0.4180  avgPre: 0.5685\n","Batch: [0088/0300]  training loss: 1.0847  meta_loss: 1.4854  hLoss: 0.1954  rLoss: 0.2319  oError: 0.4477  conv: 0.3993  avgPre: 0.6376\n","Batch: [0089/0300]  training loss: 1.0953  meta_loss: 1.5406  hLoss: 0.2052  rLoss: 0.2474  oError: 0.4535  conv: 0.4153  avgPre: 0.6056\n","Batch: [0090/0300]  training loss: 1.1173  meta_loss: 1.5494  hLoss: 0.2163  rLoss: 0.2524  oError: 0.5055  conv: 0.4158  avgPre: 0.5925\n","Batch: [0091/0300]  training loss: 1.0985  meta_loss: 1.4995  hLoss: 0.1998  rLoss: 0.2540  oError: 0.4623  conv: 0.4167  avgPre: 0.6097\n","Batch: [0092/0300]  training loss: 1.1261  meta_loss: 1.5852  hLoss: 0.2356  rLoss: 0.2929  oError: 0.5567  conv: 0.4420  avgPre: 0.5404\n","Batch: [0093/0300]  training loss: 1.1613  meta_loss: 1.6632  hLoss: 0.2115  rLoss: 0.2399  oError: 0.5121  conv: 0.4029  avgPre: 0.6114\n","Batch: [0094/0300]  training loss: 1.1058  meta_loss: 1.5809  hLoss: 0.2036  rLoss: 0.2505  oError: 0.4784  conv: 0.4177  avgPre: 0.6003\n","Batch: [0095/0300]  training loss: 1.1167  meta_loss: 1.6623  hLoss: 0.2139  rLoss: 0.2510  oError: 0.5377  conv: 0.4131  avgPre: 0.5818\n","Batch: [0096/0300]  training loss: 1.1365  meta_loss: 1.6616  hLoss: 0.2185  rLoss: 0.2645  oError: 0.5472  conv: 0.4416  avgPre: 0.5745\n","Batch: [0097/0300]  training loss: 1.1855  meta_loss: 1.8375  hLoss: 0.2118  rLoss: 0.2570  oError: 0.4974  conv: 0.4297  avgPre: 0.5809\n","Batch: [0098/0300]  training loss: 1.1042  meta_loss: 1.6382  hLoss: 0.2044  rLoss: 0.2383  oError: 0.4894  conv: 0.3995  avgPre: 0.6046\n","Batch: [0099/0300]  training loss: 1.0726  meta_loss: 1.4158  hLoss: 0.2099  rLoss: 0.2427  oError: 0.4901  conv: 0.4031  avgPre: 0.6195\n","Batch: [0100/0300]  training loss: 1.1018  meta_loss: 1.5940  hLoss: 0.2030  rLoss: 0.2394  oError: 0.4645  conv: 0.4081  avgPre: 0.6107\n","Batch: [0101/0300]  training loss: 1.1046  meta_loss: 1.4694  hLoss: 0.2011  rLoss: 0.2449  oError: 0.4740  conv: 0.4019  avgPre: 0.6208\n","Batch: [0102/0300]  training loss: 1.1640  meta_loss: 1.7544  hLoss: 0.2040  rLoss: 0.2512  oError: 0.5026  conv: 0.4154  avgPre: 0.5921\n","Batch: [0103/0300]  training loss: 1.1311  meta_loss: 1.5636  hLoss: 0.2059  rLoss: 0.2619  oError: 0.5062  conv: 0.4373  avgPre: 0.5837\n","Batch: [0104/0300]  training loss: 1.0819  meta_loss: 1.4480  hLoss: 0.2018  rLoss: 0.2310  oError: 0.4572  conv: 0.3898  avgPre: 0.6335\n","Batch: [0105/0300]  training loss: 1.1206  meta_loss: 1.5772  hLoss: 0.2266  rLoss: 0.2547  oError: 0.6174  conv: 0.4071  avgPre: 0.5700\n","Batch: [0106/0300]  training loss: 1.1253  meta_loss: 1.5205  hLoss: 0.2195  rLoss: 0.2574  oError: 0.5486  conv: 0.4160  avgPre: 0.5767\n","Batch: [0107/0300]  training loss: 1.1234  meta_loss: 1.6166  hLoss: 0.2216  rLoss: 0.2460  oError: 0.5274  conv: 0.4101  avgPre: 0.5893\n","Batch: [0108/0300]  training loss: 1.1289  meta_loss: 1.6480  hLoss: 0.2010  rLoss: 0.2624  oError: 0.4828  conv: 0.4313  avgPre: 0.6010\n","Batch: [0109/0300]  training loss: 1.1364  meta_loss: 1.7848  hLoss: 0.2018  rLoss: 0.2597  oError: 0.4901  conv: 0.4346  avgPre: 0.5900\n","Batch: [0110/0300]  training loss: 1.0998  meta_loss: 1.5740  hLoss: 0.2039  rLoss: 0.2492  oError: 0.4887  conv: 0.4164  avgPre: 0.6004\n","Batch: [0111/0300]  training loss: 1.0885  meta_loss: 1.5698  hLoss: 0.2010  rLoss: 0.2458  oError: 0.4733  conv: 0.4309  avgPre: 0.6036\n","Batch: [0112/0300]  training loss: 1.1355  meta_loss: 1.6413  hLoss: 0.2057  rLoss: 0.2339  oError: 0.4448  conv: 0.4074  avgPre: 0.6319\n","Batch: [0113/0300]  training loss: 1.0821  meta_loss: 1.5172  hLoss: 0.2084  rLoss: 0.2358  oError: 0.4623  conv: 0.4050  avgPre: 0.6243\n","Batch: [0114/0300]  training loss: 1.1209  meta_loss: 1.6221  hLoss: 0.2148  rLoss: 0.2437  oError: 0.5326  conv: 0.4088  avgPre: 0.5924\n","Batch: [0115/0300]  training loss: 1.1167  meta_loss: 1.6206  hLoss: 0.2049  rLoss: 0.2642  oError: 0.4982  conv: 0.4277  avgPre: 0.5824\n","Batch: [0116/0300]  training loss: 1.0906  meta_loss: 1.4986  hLoss: 0.2015  rLoss: 0.2415  oError: 0.4879  conv: 0.4157  avgPre: 0.6012\n","Batch: [0117/0300]  training loss: 1.1074  meta_loss: 1.6160  hLoss: 0.2045  rLoss: 0.2464  oError: 0.4901  conv: 0.4150  avgPre: 0.5975\n","Batch: [0118/0300]  training loss: 1.1192  meta_loss: 1.6468  hLoss: 0.2123  rLoss: 0.2785  oError: 0.5223  conv: 0.4434  avgPre: 0.5634\n","Batch: [0119/0300]  training loss: 1.1317  meta_loss: 1.7006  hLoss: 0.2545  rLoss: 0.2804  oError: 0.7359  conv: 0.4472  avgPre: 0.5192\n","Batch: [0120/0300]  training loss: 1.1913  meta_loss: 1.8073  hLoss: 0.2092  rLoss: 0.2338  oError: 0.4887  conv: 0.4018  avgPre: 0.6202\n","Batch: [0121/0300]  training loss: 1.0866  meta_loss: 1.5195  hLoss: 0.2315  rLoss: 0.2497  oError: 0.5735  conv: 0.4063  avgPre: 0.5890\n","Batch: [0122/0300]  training loss: 1.1386  meta_loss: 1.6262  hLoss: 0.2101  rLoss: 0.2500  oError: 0.5121  conv: 0.4144  avgPre: 0.6037\n","Batch: [0123/0300]  training loss: 1.1180  meta_loss: 1.6656  hLoss: 0.2205  rLoss: 0.2673  oError: 0.5955  conv: 0.4252  avgPre: 0.5629\n","Batch: [0124/0300]  training loss: 1.1480  meta_loss: 1.6928  hLoss: 0.2131  rLoss: 0.2468  oError: 0.5179  conv: 0.4141  avgPre: 0.5875\n","Batch: [0125/0300]  training loss: 1.1060  meta_loss: 1.5232  hLoss: 0.1967  rLoss: 0.2335  oError: 0.4418  conv: 0.4103  avgPre: 0.6357\n","Batch: [0126/0300]  training loss: 1.1092  meta_loss: 1.6853  hLoss: 0.2008  rLoss: 0.2432  oError: 0.4572  conv: 0.4295  avgPre: 0.6247\n","Batch: [0127/0300]  training loss: 1.0821  meta_loss: 1.5966  hLoss: 0.2076  rLoss: 0.2322  oError: 0.4316  conv: 0.3976  avgPre: 0.6354\n","Batch: [0128/0300]  training loss: 1.1040  meta_loss: 1.5778  hLoss: 0.2348  rLoss: 0.2583  oError: 0.5750  conv: 0.4146  avgPre: 0.5584\n","Batch: [0129/0300]  training loss: 1.1288  meta_loss: 1.6861  hLoss: 0.1973  rLoss: 0.2425  oError: 0.4499  conv: 0.4214  avgPre: 0.6156\n","Batch: [0130/0300]  training loss: 1.1128  meta_loss: 1.5681  hLoss: 0.2111  rLoss: 0.2558  oError: 0.5113  conv: 0.4176  avgPre: 0.5933\n","Batch: [0131/0300]  training loss: 1.1114  meta_loss: 1.5491  hLoss: 0.2195  rLoss: 0.2723  oError: 0.5582  conv: 0.4345  avgPre: 0.5624\n","Batch: [0132/0300]  training loss: 1.1286  meta_loss: 1.6522  hLoss: 0.2070  rLoss: 0.2424  oError: 0.4565  conv: 0.4099  avgPre: 0.6155\n","Batch: [0133/0300]  training loss: 1.1241  meta_loss: 1.6282  hLoss: 0.2096  rLoss: 0.2400  oError: 0.4755  conv: 0.4056  avgPre: 0.6228\n","Batch: [0134/0300]  training loss: 1.1145  meta_loss: 1.5899  hLoss: 0.2033  rLoss: 0.2255  oError: 0.4389  conv: 0.3904  avgPre: 0.6390\n","Batch: [0135/0300]  training loss: 1.1101  meta_loss: 1.6084  hLoss: 0.2084  rLoss: 0.2521  oError: 0.4974  conv: 0.4111  avgPre: 0.5997\n","Batch: [0136/0300]  training loss: 1.1362  meta_loss: 1.6344  hLoss: 0.2051  rLoss: 0.2541  oError: 0.5004  conv: 0.4233  avgPre: 0.5934\n","Batch: [0137/0300]  training loss: 1.1399  meta_loss: 1.6749  hLoss: 0.2079  rLoss: 0.2397  oError: 0.4726  conv: 0.4037  avgPre: 0.6159\n","Batch: [0138/0300]  training loss: 1.1159  meta_loss: 1.5621  hLoss: 0.1951  rLoss: 0.2384  oError: 0.4418  conv: 0.4042  avgPre: 0.6334\n","Batch: [0139/0300]  training loss: 1.0793  meta_loss: 1.5171  hLoss: 0.2078  rLoss: 0.2337  oError: 0.4931  conv: 0.4020  avgPre: 0.6188\n","Batch: [0140/0300]  training loss: 1.0961  meta_loss: 1.5730  hLoss: 0.2147  rLoss: 0.2466  oError: 0.5391  conv: 0.4121  avgPre: 0.6008\n","Batch: [0141/0300]  training loss: 1.1061  meta_loss: 1.6525  hLoss: 0.2021  rLoss: 0.2539  oError: 0.4755  conv: 0.4243  avgPre: 0.6016\n","Batch: [0142/0300]  training loss: 1.0877  meta_loss: 1.5686  hLoss: 0.2030  rLoss: 0.2429  oError: 0.4799  conv: 0.4169  avgPre: 0.6054\n","Batch: [0143/0300]  training loss: 1.1219  meta_loss: 1.6082  hLoss: 0.2139  rLoss: 0.2703  oError: 0.5677  conv: 0.4293  avgPre: 0.5610\n","Batch: [0144/0300]  training loss: 1.1084  meta_loss: 1.6225  hLoss: 0.1966  rLoss: 0.2247  oError: 0.4265  conv: 0.3934  avgPre: 0.6478\n","Batch: [0145/0300]  training loss: 1.1165  meta_loss: 1.6373  hLoss: 0.2068  rLoss: 0.2401  oError: 0.4704  conv: 0.4072  avgPre: 0.6248\n","Batch: [0146/0300]  training loss: 1.1235  meta_loss: 1.5819  hLoss: 0.1988  rLoss: 0.2389  oError: 0.4631  conv: 0.4125  avgPre: 0.6124\n","Batch: [0147/0300]  training loss: 1.1367  meta_loss: 1.6352  hLoss: 0.1992  rLoss: 0.2321  oError: 0.4294  conv: 0.4067  avgPre: 0.6308\n","Batch: [0148/0300]  training loss: 1.1056  meta_loss: 1.5988  hLoss: 0.1948  rLoss: 0.2399  oError: 0.4367  conv: 0.4100  avgPre: 0.6245\n","Batch: [0149/0300]  training loss: 1.1132  meta_loss: 1.6090  hLoss: 0.2089  rLoss: 0.2400  oError: 0.4674  conv: 0.4034  avgPre: 0.6220\n","Batch: [0150/0300]  training loss: 1.1070  meta_loss: 1.5846  hLoss: 0.2043  rLoss: 0.2455  oError: 0.4967  conv: 0.4119  avgPre: 0.6045\n","Batch: [0151/0300]  training loss: 1.1025  meta_loss: 1.6028  hLoss: 0.2083  rLoss: 0.2260  oError: 0.4338  conv: 0.3991  avgPre: 0.6372\n","Batch: [0152/0300]  training loss: 1.0834  meta_loss: 1.4837  hLoss: 0.2076  rLoss: 0.2317  oError: 0.4345  conv: 0.3929  avgPre: 0.6380\n","Batch: [0153/0300]  training loss: 1.0928  meta_loss: 1.5362  hLoss: 0.2060  rLoss: 0.2441  oError: 0.4477  conv: 0.4111  avgPre: 0.6126\n","Batch: [0154/0300]  training loss: 1.1574  meta_loss: 1.6759  hLoss: 0.2103  rLoss: 0.2530  oError: 0.4792  conv: 0.4105  avgPre: 0.6086\n","Batch: [0155/0300]  training loss: 1.0858  meta_loss: 1.4982  hLoss: 0.1945  rLoss: 0.2344  oError: 0.4484  conv: 0.4034  avgPre: 0.6304\n","Batch: [0156/0300]  training loss: 1.0873  meta_loss: 1.5604  hLoss: 0.2092  rLoss: 0.2318  oError: 0.4799  conv: 0.3982  avgPre: 0.6306\n","Batch: [0157/0300]  training loss: 1.1021  meta_loss: 1.5736  hLoss: 0.2008  rLoss: 0.2426  oError: 0.4857  conv: 0.4063  avgPre: 0.6026\n","Batch: [0158/0300]  training loss: 1.1175  meta_loss: 1.5937  hLoss: 0.2051  rLoss: 0.2385  oError: 0.4967  conv: 0.4079  avgPre: 0.6041\n","Batch: [0159/0300]  training loss: 1.1140  meta_loss: 1.5955  hLoss: 0.2056  rLoss: 0.2434  oError: 0.4982  conv: 0.4073  avgPre: 0.5992\n","Batch: [0160/0300]  training loss: 1.1074  meta_loss: 1.6718  hLoss: 0.2120  rLoss: 0.2435  oError: 0.5187  conv: 0.3970  avgPre: 0.6076\n","Batch: [0161/0300]  training loss: 1.1012  meta_loss: 1.6442  hLoss: 0.2019  rLoss: 0.2495  oError: 0.4718  conv: 0.4148  avgPre: 0.6069\n","Batch: [0162/0300]  training loss: 1.0665  meta_loss: 1.5426  hLoss: 0.2049  rLoss: 0.2254  oError: 0.4257  conv: 0.3958  avgPre: 0.6360\n","Batch: [0163/0300]  training loss: 1.0986  meta_loss: 1.5388  hLoss: 0.2173  rLoss: 0.2565  oError: 0.5143  conv: 0.4178  avgPre: 0.5935\n","Batch: [0164/0300]  training loss: 1.1046  meta_loss: 1.6061  hLoss: 0.1940  rLoss: 0.2312  oError: 0.4411  conv: 0.4005  avgPre: 0.6263\n","Batch: [0165/0300]  training loss: 1.1180  meta_loss: 1.5834  hLoss: 0.1962  rLoss: 0.2357  oError: 0.4082  conv: 0.4136  avgPre: 0.6340\n","Batch: [0166/0300]  training loss: 1.1010  meta_loss: 1.5382  hLoss: 0.2042  rLoss: 0.2304  oError: 0.4309  conv: 0.4006  avgPre: 0.6396\n","Batch: [0167/0300]  training loss: 1.0649  meta_loss: 1.3943  hLoss: 0.2004  rLoss: 0.2426  oError: 0.4704  conv: 0.4047  avgPre: 0.6223\n","Batch: [0168/0300]  training loss: 1.1283  meta_loss: 1.6964  hLoss: 0.2090  rLoss: 0.2588  oError: 0.5048  conv: 0.4198  avgPre: 0.5877\n","Batch: [0169/0300]  training loss: 1.0696  meta_loss: 1.4368  hLoss: 0.1977  rLoss: 0.2304  oError: 0.4448  conv: 0.3937  avgPre: 0.6397\n","Batch: [0170/0300]  training loss: 1.0863  meta_loss: 1.5496  hLoss: 0.1984  rLoss: 0.2346  oError: 0.4345  conv: 0.4046  avgPre: 0.6303\n","Batch: [0171/0300]  training loss: 1.0895  meta_loss: 1.5247  hLoss: 0.2101  rLoss: 0.2594  oError: 0.5106  conv: 0.4124  avgPre: 0.5823\n","Batch: [0172/0300]  training loss: 1.0950  meta_loss: 1.6279  hLoss: 0.2048  rLoss: 0.2379  oError: 0.4521  conv: 0.4012  avgPre: 0.6182\n","Batch: [0173/0300]  training loss: 1.0842  meta_loss: 1.5319  hLoss: 0.1999  rLoss: 0.2299  oError: 0.4236  conv: 0.3979  avgPre: 0.6404\n","Batch: [0174/0300]  training loss: 1.1004  meta_loss: 1.5988  hLoss: 0.2033  rLoss: 0.2417  oError: 0.4777  conv: 0.4019  avgPre: 0.6054\n","Batch: [0175/0300]  training loss: 1.1100  meta_loss: 1.6033  hLoss: 0.1990  rLoss: 0.2438  oError: 0.4806  conv: 0.4078  avgPre: 0.6083\n","Batch: [0176/0300]  training loss: 1.1036  meta_loss: 1.5794  hLoss: 0.2086  rLoss: 0.2465  oError: 0.4726  conv: 0.3987  avgPre: 0.6182\n","Batch: [0177/0300]  training loss: 1.0991  meta_loss: 1.6429  hLoss: 0.2003  rLoss: 0.2390  oError: 0.4594  conv: 0.4082  avgPre: 0.6312\n","Batch: [0178/0300]  training loss: 1.1137  meta_loss: 1.5357  hLoss: 0.1947  rLoss: 0.2411  oError: 0.4418  conv: 0.4172  avgPre: 0.6240\n","Batch: [0179/0300]  training loss: 1.0885  meta_loss: 1.5603  hLoss: 0.2215  rLoss: 0.2647  oError: 0.5040  conv: 0.4300  avgPre: 0.5662\n","Batch: [0180/0300]  training loss: 1.1283  meta_loss: 1.6722  hLoss: 0.1972  rLoss: 0.2518  oError: 0.4784  conv: 0.4144  avgPre: 0.6048\n","Batch: [0181/0300]  training loss: 1.1010  meta_loss: 1.6245  hLoss: 0.2036  rLoss: 0.2320  oError: 0.4250  conv: 0.4023  avgPre: 0.6377\n","Batch: [0182/0300]  training loss: 1.1285  meta_loss: 1.5982  hLoss: 0.1992  rLoss: 0.2401  oError: 0.4726  conv: 0.4115  avgPre: 0.6153\n","Batch: [0183/0300]  training loss: 1.1132  meta_loss: 1.5978  hLoss: 0.2215  rLoss: 0.2395  oError: 0.5260  conv: 0.4065  avgPre: 0.5958\n","Batch: [0184/0300]  training loss: 1.1137  meta_loss: 1.6202  hLoss: 0.2131  rLoss: 0.2675  oError: 0.5106  conv: 0.4389  avgPre: 0.5727\n","Batch: [0185/0300]  training loss: 1.1030  meta_loss: 1.5088  hLoss: 0.2112  rLoss: 0.2430  oError: 0.4645  conv: 0.4096  avgPre: 0.6145\n","Batch: [0186/0300]  training loss: 1.1200  meta_loss: 1.6548  hLoss: 0.2032  rLoss: 0.2408  oError: 0.4682  conv: 0.4085  avgPre: 0.6027\n","Batch: [0187/0300]  training loss: 1.0950  meta_loss: 1.5705  hLoss: 0.1937  rLoss: 0.2318  oError: 0.4448  conv: 0.3987  avgPre: 0.6245\n","Batch: [0188/0300]  training loss: 1.0899  meta_loss: 1.5350  hLoss: 0.2310  rLoss: 0.2659  oError: 0.5501  conv: 0.4323  avgPre: 0.5543\n","Batch: [0189/0300]  training loss: 1.0946  meta_loss: 1.5952  hLoss: 0.2020  rLoss: 0.2420  oError: 0.4682  conv: 0.4106  avgPre: 0.6113\n","Batch: [0190/0300]  training loss: 1.1125  meta_loss: 1.6482  hLoss: 0.2050  rLoss: 0.2565  oError: 0.5208  conv: 0.4236  avgPre: 0.5789\n","Batch: [0191/0300]  training loss: 1.1198  meta_loss: 1.6280  hLoss: 0.1992  rLoss: 0.2335  oError: 0.4528  conv: 0.4095  avgPre: 0.6321\n","Batch: [0192/0300]  training loss: 1.1230  meta_loss: 1.6603  hLoss: 0.2171  rLoss: 0.2829  oError: 0.5684  conv: 0.4446  avgPre: 0.5464\n","Batch: [0193/0300]  training loss: 1.1164  meta_loss: 1.7036  hLoss: 0.2101  rLoss: 0.2510  oError: 0.5033  conv: 0.4201  avgPre: 0.5937\n","Batch: [0194/0300]  training loss: 1.1388  meta_loss: 1.6962  hLoss: 0.1980  rLoss: 0.2413  oError: 0.4645  conv: 0.3983  avgPre: 0.6258\n","Batch: [0195/0300]  training loss: 1.1003  meta_loss: 1.5650  hLoss: 0.2024  rLoss: 0.2412  oError: 0.4484  conv: 0.4131  avgPre: 0.6245\n","Batch: [0196/0300]  training loss: 1.0793  meta_loss: 1.4455  hLoss: 0.2068  rLoss: 0.2595  oError: 0.4989  conv: 0.4267  avgPre: 0.5877\n","Batch: [0197/0300]  training loss: 1.1420  meta_loss: 1.6708  hLoss: 0.2113  rLoss: 0.2497  oError: 0.5077  conv: 0.4162  avgPre: 0.5917\n","Batch: [0198/0300]  training loss: 1.1314  meta_loss: 1.6712  hLoss: 0.2059  rLoss: 0.2397  oError: 0.4609  conv: 0.4142  avgPre: 0.6165\n","Batch: [0199/0300]  training loss: 1.0924  meta_loss: 1.5161  hLoss: 0.2071  rLoss: 0.2313  oError: 0.4550  conv: 0.3949  avgPre: 0.6274\n","Batch: [0200/0300]  training loss: 1.0709  meta_loss: 1.4332  hLoss: 0.2028  rLoss: 0.2557  oError: 0.4843  conv: 0.4207  avgPre: 0.5995\n","Batch: [0201/0300]  training loss: 1.1421  meta_loss: 1.7394  hLoss: 0.1999  rLoss: 0.2622  oError: 0.4982  conv: 0.4279  avgPre: 0.5892\n","Batch: [0202/0300]  training loss: 1.1509  meta_loss: 1.7378  hLoss: 0.2153  rLoss: 0.2681  oError: 0.5750  conv: 0.4288  avgPre: 0.5670\n","Batch: [0203/0300]  training loss: 1.1104  meta_loss: 1.6613  hLoss: 0.2011  rLoss: 0.2380  oError: 0.4565  conv: 0.4028  avgPre: 0.6322\n","Batch: [0204/0300]  training loss: 1.0583  meta_loss: 1.4919  hLoss: 0.2117  rLoss: 0.2406  oError: 0.5201  conv: 0.4013  avgPre: 0.5988\n","Batch: [0205/0300]  training loss: 1.1028  meta_loss: 1.5820  hLoss: 0.2081  rLoss: 0.2424  oError: 0.4777  conv: 0.4011  avgPre: 0.6163\n","Batch: [0206/0300]  training loss: 1.1120  meta_loss: 1.6237  hLoss: 0.2040  rLoss: 0.2663  oError: 0.5399  conv: 0.4262  avgPre: 0.5757\n","Batch: [0207/0300]  training loss: 1.1043  meta_loss: 1.5494  hLoss: 0.1999  rLoss: 0.2283  oError: 0.4119  conv: 0.4030  avgPre: 0.6415\n","Batch: [0208/0300]  training loss: 1.1012  meta_loss: 1.5790  hLoss: 0.2187  rLoss: 0.2493  oError: 0.4733  conv: 0.4177  avgPre: 0.5971\n","Batch: [0209/0300]  training loss: 1.1278  meta_loss: 1.6675  hLoss: 0.1984  rLoss: 0.2492  oError: 0.4726  conv: 0.4136  avgPre: 0.6150\n","Batch: [0210/0300]  training loss: 1.1017  meta_loss: 1.6250  hLoss: 0.2079  rLoss: 0.2496  oError: 0.4718  conv: 0.4183  avgPre: 0.6020\n","Batch: [0211/0300]  training loss: 1.1200  meta_loss: 1.7130  hLoss: 0.2076  rLoss: 0.2284  oError: 0.4777  conv: 0.3917  avgPre: 0.6294\n","Batch: [0212/0300]  training loss: 1.1242  meta_loss: 1.6115  hLoss: 0.2097  rLoss: 0.2352  oError: 0.4492  conv: 0.4013  avgPre: 0.6266\n","Batch: [0213/0300]  training loss: 1.1591  meta_loss: 1.7394  hLoss: 0.2146  rLoss: 0.2661  oError: 0.5340  conv: 0.4248  avgPre: 0.5604\n","Batch: [0214/0300]  training loss: 1.0920  meta_loss: 1.6078  hLoss: 0.2024  rLoss: 0.2271  oError: 0.4521  conv: 0.3983  avgPre: 0.6401\n","Batch: [0215/0300]  training loss: 1.0877  meta_loss: 1.5449  hLoss: 0.2020  rLoss: 0.2398  oError: 0.4806  conv: 0.4070  avgPre: 0.6082\n","Batch: [0216/0300]  training loss: 1.0721  meta_loss: 1.5576  hLoss: 0.2028  rLoss: 0.2522  oError: 0.5216  conv: 0.4114  avgPre: 0.5904\n","Batch: [0217/0300]  training loss: 1.1050  meta_loss: 1.4510  hLoss: 0.2090  rLoss: 0.2539  oError: 0.4952  conv: 0.4229  avgPre: 0.5967\n","Batch: [0218/0300]  training loss: 1.1156  meta_loss: 1.5917  hLoss: 0.1937  rLoss: 0.2339  oError: 0.4265  conv: 0.4071  avgPre: 0.6278\n","Batch: [0219/0300]  training loss: 1.1613  meta_loss: 1.7441  hLoss: 0.2260  rLoss: 0.2539  oError: 0.5560  conv: 0.4156  avgPre: 0.5713\n","Batch: [0220/0300]  training loss: 1.1159  meta_loss: 1.6205  hLoss: 0.2034  rLoss: 0.2458  oError: 0.4879  conv: 0.4058  avgPre: 0.6034\n","Batch: [0221/0300]  training loss: 1.0931  meta_loss: 1.5666  hLoss: 0.2114  rLoss: 0.2537  oError: 0.5084  conv: 0.4286  avgPre: 0.5940\n","Batch: [0222/0300]  training loss: 1.1733  meta_loss: 1.7681  hLoss: 0.2025  rLoss: 0.2594  oError: 0.4989  conv: 0.4244  avgPre: 0.5901\n","Batch: [0223/0300]  training loss: 1.1165  meta_loss: 1.6980  hLoss: 0.1922  rLoss: 0.2308  oError: 0.4228  conv: 0.4037  avgPre: 0.6357\n","Batch: [0224/0300]  training loss: 1.0843  meta_loss: 1.5506  hLoss: 0.2071  rLoss: 0.2462  oError: 0.4857  conv: 0.4199  avgPre: 0.6023\n","Batch: [0225/0300]  training loss: 1.1221  meta_loss: 1.6713  hLoss: 0.2107  rLoss: 0.2537  oError: 0.5406  conv: 0.4182  avgPre: 0.5875\n","Batch: [0226/0300]  training loss: 1.0990  meta_loss: 1.5682  hLoss: 0.2072  rLoss: 0.2456  oError: 0.4696  conv: 0.4253  avgPre: 0.6142\n","Batch: [0227/0300]  training loss: 1.1130  meta_loss: 1.5660  hLoss: 0.2110  rLoss: 0.2579  oError: 0.5377  conv: 0.4194  avgPre: 0.5787\n","Batch: [0228/0300]  training loss: 1.0990  meta_loss: 1.5381  hLoss: 0.2035  rLoss: 0.2270  oError: 0.4250  conv: 0.4019  avgPre: 0.6393\n","Batch: [0229/0300]  training loss: 1.0919  meta_loss: 1.5466  hLoss: 0.2017  rLoss: 0.2467  oError: 0.4674  conv: 0.4228  avgPre: 0.6030\n","Batch: [0230/0300]  training loss: 1.0684  meta_loss: 1.5882  hLoss: 0.1998  rLoss: 0.2334  oError: 0.4682  conv: 0.3941  avgPre: 0.6310\n","Batch: [0231/0300]  training loss: 1.1017  meta_loss: 1.5879  hLoss: 0.2010  rLoss: 0.2291  oError: 0.4543  conv: 0.3945  avgPre: 0.6372\n","Batch: [0232/0300]  training loss: 1.0837  meta_loss: 1.5464  hLoss: 0.2223  rLoss: 0.2477  oError: 0.5384  conv: 0.4098  avgPre: 0.5800\n","Batch: [0233/0300]  training loss: 1.1016  meta_loss: 1.5521  hLoss: 0.2074  rLoss: 0.2453  oError: 0.4645  conv: 0.4094  avgPre: 0.6089\n","Batch: [0234/0300]  training loss: 1.0888  meta_loss: 1.5378  hLoss: 0.2114  rLoss: 0.2353  oError: 0.4645  conv: 0.4023  avgPre: 0.6245\n","Batch: [0235/0300]  training loss: 1.0750  meta_loss: 1.5184  hLoss: 0.2038  rLoss: 0.2533  oError: 0.4857  conv: 0.4228  avgPre: 0.5889\n","Batch: [0236/0300]  training loss: 1.0809  meta_loss: 1.5120  hLoss: 0.2028  rLoss: 0.2547  oError: 0.5018  conv: 0.4165  avgPre: 0.5939\n","Batch: [0237/0300]  training loss: 1.1121  meta_loss: 1.6009  hLoss: 0.2121  rLoss: 0.2436  oError: 0.4996  conv: 0.4069  avgPre: 0.6014\n","Batch: [0238/0300]  training loss: 1.1526  meta_loss: 1.6573  hLoss: 0.2019  rLoss: 0.2596  oError: 0.4974  conv: 0.4273  avgPre: 0.5989\n","Batch: [0239/0300]  training loss: 1.1009  meta_loss: 1.6286  hLoss: 0.2053  rLoss: 0.2230  oError: 0.4038  conv: 0.3993  avgPre: 0.6477\n","Batch: [0240/0300]  training loss: 1.0815  meta_loss: 1.5142  hLoss: 0.2106  rLoss: 0.2450  oError: 0.5625  conv: 0.4062  avgPre: 0.5909\n","Batch: [0241/0300]  training loss: 1.0991  meta_loss: 1.5765  hLoss: 0.1994  rLoss: 0.2303  oError: 0.4674  conv: 0.3940  avgPre: 0.6327\n","Batch: [0242/0300]  training loss: 1.0912  meta_loss: 1.5620  hLoss: 0.2002  rLoss: 0.2416  oError: 0.4784  conv: 0.4004  avgPre: 0.6134\n","Batch: [0243/0300]  training loss: 1.1011  meta_loss: 1.5601  hLoss: 0.2025  rLoss: 0.2400  oError: 0.4426  conv: 0.4135  avgPre: 0.6163\n","Batch: [0244/0300]  training loss: 1.1026  meta_loss: 1.5671  hLoss: 0.2079  rLoss: 0.2332  oError: 0.4389  conv: 0.3993  avgPre: 0.6300\n","Batch: [0245/0300]  training loss: 1.1088  meta_loss: 1.6070  hLoss: 0.2080  rLoss: 0.2487  oError: 0.4982  conv: 0.4142  avgPre: 0.5992\n","Batch: [0246/0300]  training loss: 1.1028  meta_loss: 1.6020  hLoss: 0.2143  rLoss: 0.2490  oError: 0.5135  conv: 0.4088  avgPre: 0.5918\n","Batch: [0247/0300]  training loss: 1.1521  meta_loss: 1.7201  hLoss: 0.2288  rLoss: 0.2570  oError: 0.5435  conv: 0.4090  avgPre: 0.5886\n","Batch: [0248/0300]  training loss: 1.1424  meta_loss: 1.6504  hLoss: 0.2193  rLoss: 0.2445  oError: 0.5457  conv: 0.4111  avgPre: 0.5862\n","Batch: [0249/0300]  training loss: 1.1257  meta_loss: 1.6994  hLoss: 0.2127  rLoss: 0.2347  oError: 0.4777  conv: 0.4002  avgPre: 0.6268\n","Batch: [0250/0300]  training loss: 1.0782  meta_loss: 1.5010  hLoss: 0.1957  rLoss: 0.2353  oError: 0.4492  conv: 0.4013  avgPre: 0.6287\n","Batch: [0251/0300]  training loss: 1.1044  meta_loss: 1.5549  hLoss: 0.2127  rLoss: 0.2582  oError: 0.5267  conv: 0.4281  avgPre: 0.5714\n","Batch: [0252/0300]  training loss: 1.0917  meta_loss: 1.5032  hLoss: 0.2043  rLoss: 0.2464  oError: 0.4660  conv: 0.4101  avgPre: 0.6121\n","Batch: [0253/0300]  training loss: 1.0982  meta_loss: 1.5827  hLoss: 0.2197  rLoss: 0.2646  oError: 0.5274  conv: 0.4183  avgPre: 0.5843\n","Batch: [0254/0300]  training loss: 1.1175  meta_loss: 1.5920  hLoss: 0.2047  rLoss: 0.2255  oError: 0.4448  conv: 0.3914  avgPre: 0.6401\n","Batch: [0255/0300]  training loss: 1.0931  meta_loss: 1.5969  hLoss: 0.2113  rLoss: 0.2378  oError: 0.4828  conv: 0.3986  avgPre: 0.6197\n","Batch: [0256/0300]  training loss: 1.0878  meta_loss: 1.5371  hLoss: 0.2016  rLoss: 0.2322  oError: 0.4521  conv: 0.4013  avgPre: 0.6229\n","Batch: [0257/0300]  training loss: 1.0982  meta_loss: 1.5523  hLoss: 0.1981  rLoss: 0.2465  oError: 0.4609  conv: 0.4144  avgPre: 0.6111\n","Batch: [0258/0300]  training loss: 1.1256  meta_loss: 1.5615  hLoss: 0.2139  rLoss: 0.2754  oError: 0.5311  conv: 0.4377  avgPre: 0.5587\n","Batch: [0259/0300]  training loss: 1.1373  meta_loss: 1.6535  hLoss: 0.2025  rLoss: 0.2347  oError: 0.4528  conv: 0.4126  avgPre: 0.6370\n","Batch: [0260/0300]  training loss: 1.1433  meta_loss: 1.7126  hLoss: 0.2185  rLoss: 0.2480  oError: 0.4982  conv: 0.4182  avgPre: 0.5950\n","Batch: [0261/0300]  training loss: 1.1577  meta_loss: 1.7439  hLoss: 0.2190  rLoss: 0.2510  oError: 0.5465  conv: 0.4110  avgPre: 0.5907\n","Batch: [0262/0300]  training loss: 1.0772  meta_loss: 1.5266  hLoss: 0.2043  rLoss: 0.2422  oError: 0.4470  conv: 0.4106  avgPre: 0.6251\n","Batch: [0263/0300]  training loss: 1.1116  meta_loss: 1.5794  hLoss: 0.2074  rLoss: 0.2517  oError: 0.4931  conv: 0.4195  avgPre: 0.5997\n","Batch: [0264/0300]  training loss: 1.0996  meta_loss: 1.6081  hLoss: 0.2119  rLoss: 0.2500  oError: 0.4770  conv: 0.4113  avgPre: 0.5945\n","Batch: [0265/0300]  training loss: 1.0911  meta_loss: 1.5792  hLoss: 0.2084  rLoss: 0.2493  oError: 0.4938  conv: 0.4109  avgPre: 0.5953\n","Batch: [0266/0300]  training loss: 1.0841  meta_loss: 1.5954  hLoss: 0.2074  rLoss: 0.2535  oError: 0.5267  conv: 0.4145  avgPre: 0.5940\n","Batch: [0267/0300]  training loss: 1.1120  meta_loss: 1.6089  hLoss: 0.2042  rLoss: 0.2321  oError: 0.4857  conv: 0.3954  avgPre: 0.6108\n","Batch: [0268/0300]  training loss: 1.1055  meta_loss: 1.5991  hLoss: 0.2213  rLoss: 0.2844  oError: 0.5296  conv: 0.4428  avgPre: 0.5435\n","Batch: [0269/0300]  training loss: 1.1523  meta_loss: 1.7281  hLoss: 0.2157  rLoss: 0.2542  oError: 0.5838  conv: 0.4129  avgPre: 0.5727\n","Batch: [0270/0300]  training loss: 1.1082  meta_loss: 1.5529  hLoss: 0.2018  rLoss: 0.2414  oError: 0.4799  conv: 0.4122  avgPre: 0.6087\n","Batch: [0271/0300]  training loss: 1.0913  meta_loss: 1.5565  hLoss: 0.1991  rLoss: 0.2468  oError: 0.4777  conv: 0.4072  avgPre: 0.6172\n","Batch: [0272/0300]  training loss: 1.1000  meta_loss: 1.5729  hLoss: 0.2022  rLoss: 0.2444  oError: 0.4433  conv: 0.4097  avgPre: 0.6189\n","Batch: [0273/0300]  training loss: 1.0994  meta_loss: 1.6322  hLoss: 0.2093  rLoss: 0.2533  oError: 0.5143  conv: 0.4229  avgPre: 0.5910\n","Batch: [0274/0300]  training loss: 1.0813  meta_loss: 1.4971  hLoss: 0.2016  rLoss: 0.2343  oError: 0.4484  conv: 0.4079  avgPre: 0.6335\n","Batch: [0275/0300]  training loss: 1.1286  meta_loss: 1.6774  hLoss: 0.2105  rLoss: 0.2635  oError: 0.5121  conv: 0.4190  avgPre: 0.5919\n","Batch: [0276/0300]  training loss: 1.1264  meta_loss: 1.5844  hLoss: 0.2038  rLoss: 0.2396  oError: 0.5062  conv: 0.4026  avgPre: 0.6032\n","Batch: [0277/0300]  training loss: 1.1139  meta_loss: 1.6369  hLoss: 0.2042  rLoss: 0.2411  oError: 0.4514  conv: 0.4196  avgPre: 0.6169\n","Batch: [0278/0300]  training loss: 1.0853  meta_loss: 1.4918  hLoss: 0.1957  rLoss: 0.2314  oError: 0.4345  conv: 0.3978  avgPre: 0.6432\n","Batch: [0279/0300]  training loss: 1.1427  meta_loss: 1.5867  hLoss: 0.2055  rLoss: 0.2440  oError: 0.4909  conv: 0.4083  avgPre: 0.6021\n","Batch: [0280/0300]  training loss: 1.1165  meta_loss: 1.5606  hLoss: 0.2122  rLoss: 0.2519  oError: 0.5011  conv: 0.4178  avgPre: 0.5974\n","Batch: [0281/0300]  training loss: 1.1282  meta_loss: 1.6276  hLoss: 0.1964  rLoss: 0.2312  oError: 0.4389  conv: 0.3991  avgPre: 0.6262\n","Batch: [0282/0300]  training loss: 1.1188  meta_loss: 1.6396  hLoss: 0.2064  rLoss: 0.2545  oError: 0.5589  conv: 0.4146  avgPre: 0.5785\n","Batch: [0283/0300]  training loss: 1.1239  meta_loss: 1.5820  hLoss: 0.1939  rLoss: 0.2284  oError: 0.4257  conv: 0.3982  avgPre: 0.6354\n","Batch: [0284/0300]  training loss: 1.0876  meta_loss: 1.5701  hLoss: 0.2065  rLoss: 0.2530  oError: 0.4967  conv: 0.4243  avgPre: 0.5906\n","Batch: [0285/0300]  training loss: 1.0887  meta_loss: 1.5720  hLoss: 0.2054  rLoss: 0.2315  oError: 0.4660  conv: 0.3972  avgPre: 0.6205\n","Batch: [0286/0300]  training loss: 1.1055  meta_loss: 1.5855  hLoss: 0.2129  rLoss: 0.2441  oError: 0.4967  conv: 0.4103  avgPre: 0.5933\n","Batch: [0287/0300]  training loss: 1.1097  meta_loss: 1.5525  hLoss: 0.2061  rLoss: 0.2374  oError: 0.4704  conv: 0.4081  avgPre: 0.6114\n","Batch: [0288/0300]  training loss: 1.0783  meta_loss: 1.4969  hLoss: 0.2062  rLoss: 0.2444  oError: 0.4594  conv: 0.4153  avgPre: 0.6144\n","Batch: [0289/0300]  training loss: 1.0893  meta_loss: 1.5703  hLoss: 0.1958  rLoss: 0.2464  oError: 0.4353  conv: 0.4044  avgPre: 0.6271\n","Batch: [0290/0300]  training loss: 1.1101  meta_loss: 1.6016  hLoss: 0.1992  rLoss: 0.2443  oError: 0.4470  conv: 0.4103  avgPre: 0.6234\n","Batch: [0291/0300]  training loss: 1.0852  meta_loss: 1.5992  hLoss: 0.2088  rLoss: 0.2307  oError: 0.4638  conv: 0.4019  avgPre: 0.6261\n","Batch: [0292/0300]  training loss: 1.1180  meta_loss: 1.6224  hLoss: 0.2117  rLoss: 0.2545  oError: 0.4879  conv: 0.4337  avgPre: 0.5909\n","Batch: [0293/0300]  training loss: 1.1647  meta_loss: 1.7503  hLoss: 0.1998  rLoss: 0.2393  oError: 0.4638  conv: 0.4096  avgPre: 0.6142\n","Batch: [0294/0300]  training loss: 1.1218  meta_loss: 1.6220  hLoss: 0.1964  rLoss: 0.2495  oError: 0.4718  conv: 0.4166  avgPre: 0.6092\n","Batch: [0295/0300]  training loss: 1.1072  meta_loss: 1.5860  hLoss: 0.2058  rLoss: 0.2387  oError: 0.4733  conv: 0.4024  avgPre: 0.6202\n","Batch: [0296/0300]  training loss: 1.0989  meta_loss: 1.5731  hLoss: 0.2037  rLoss: 0.2508  oError: 0.4587  conv: 0.4109  avgPre: 0.6116\n","Batch: [0297/0300]  training loss: 1.1445  meta_loss: 1.6863  hLoss: 0.1953  rLoss: 0.2431  oError: 0.4484  conv: 0.4185  avgPre: 0.6207\n","Batch: [0298/0300]  training loss: 1.1392  meta_loss: 1.6710  hLoss: 0.1969  rLoss: 0.2416  oError: 0.4514  conv: 0.4170  avgPre: 0.6156\n","Batch: [0299/0300]  training loss: 1.0992  meta_loss: 1.5560  hLoss: 0.2113  rLoss: 0.2421  oError: 0.4623  conv: 0.4136  avgPre: 0.6197\n","Batch: [0300/0300]  training loss: 1.0865  meta_loss: 1.5248  hLoss: 0.2233  rLoss: 0.2582  oError: 0.5940  conv: 0.4138  avgPre: 0.5743\n","\n","Test results of the last :\t Best hLoss: 0.2233  Best rLoss: 0.2582  Best oError: 0.5940  Best conv: 0.4138  Best avgPre: 0.5743 \n"]}],"source":["dir = \"/content/drive/MyDrive/Colab Notebooks\"\n","batch_size = 100\n","meta_size = 5\n","#lr_list = [0.01, 0.001]\n","lr_list = [0.03]\n","momentum = 0.9\n","weight_decay = 1e-4\n","#methods = ['meta','baseline', 'baseline_clean','ground_truth']\n","#datasets = ['music_style','mirflickr','emotions','enron','emotions','CAL500','scene','genbase']\n","#datasets  =['emotions','enron','CAL500','scene','genbase']\n","datasets = ['music_emotion']\n","method='meta' # chose from meta,baseline_clean,baseline\n","cv_start = 0\n","cv_end = 1\n","noise_list_m = [50,100,150]\n","#noise_list_m = [150]\n","true_list = [1,3,5,7,9]\n","true_list = [1,9]\n","do_save_creds = False\n","if __name__ == '__main__':\n","\n","    #for data in datasets:\n","    for data in datasets:\n","        print('\\n')\n","\n","        if data in ['music_emotion', 'music_style']:\n","            batch_size = 200\n","        elif data in ['mirflickr']:\n","            batch_size = 500\n","        elif data in ['CAL500', 'emotions','genbase']:\n","            batch_size = 50\n","        elif data in ['enron','scene']:\n","            batch_size = 100\n","\n","        if data in ['music_emotion', 'music_style','mirflickr','YeastBP']:\n","          noise_list = [0]\n","        elif data in ['emotions','enron','CAL500','scene','genbase']:\n","          noise_list = noise_list_m.copy()\n","\n","        for lr in lr_list:\n","            #for method in methods:\n","            #print('Dataname: {}\\t Methods: {}\\t Learning rate: {}'.format(data, method, lr))\n","            #result = np.empty((repeat, 5), dtype=np.float)\n","            for i in range(cv_start,cv_end):\n","                for p_noise in noise_list:\n","                    for tru in range(len(true_list)):\n","                    #for p_true in true_list:\n","                        p_true = true_list[tru]\n","                        cv_num = i\n","                        print(cv_num)\n","                        print(f'data={data}/cv={str(i)}/p_noise={str(p_noise)}/p_true={str(p_true)}/method={method}\\n')\n","                        train_loader, creds_loader, test_loader, meta_loader, clean_loader, \\\n","                        noisy_loader, features_num, labels_num = dataset.get_loader(data, batch_size, cv_num,p_noise,p_true,meta_size=meta_size)\n","\n","                        if method == 'meta':\n","                            train_data, train_target,  train_plabels, meta_features, meta_labels = get_train_data(data,p_noise,p_true,cv_num)\n","                            hamming_loss, ranking_loss, one_error, coverage, avg_precision ,outputs, p_labels ,net= metalearning(train_loader,\n","                                                                                                    test_loader,\n","                                                                                                    meta_loader,\n","                                                                                                    train_data,\n","                                                                                                    train_plabels,\n","                                                                                                    meta_features,\n","                                                                                                    meta_labels,\n","                                                                                                    data,\n","                                                                                                    p_true,\n","                                                                                                    clean=False,\n","                                                                                                    save_creds_every_epoch=True)\n","                            if do_save_creds:\n","                              p_creds = compute_p_creds(train_data,train_plabels, meta_features, meta_labels,net)\n","                              save_p_creds(data,train_target,p_noise,p_true,cv_num,p_creds)\n","\n","\n","                        elif method == 'meta_clean':\n","                            hamming_loss, ranking_loss, one_error, coverage, avg_precision ,outputs, p_labels = metalearning(train_loader,\n","                                                                                                    test_loader,\n","                                                                                                    meta_loader,\n","                                                                                                    clean=True)\n","                        elif method == 'baseline':\n","                            hamming_loss, ranking_loss, one_error, coverage, avg_precision ,outputs, p_labels = baseline(noisy_loader,\n","                                                                                                test_loader, \n","                                                                                                meta_loader,\n","                                                                                                clean=False)\n","                            \n","                        # myfunc train_loader to creds_loader\n","                        # 提案手法\n","                        elif method == 'baseline_clean':\n","                            hamming_loss, ranking_loss, one_error, coverage, avg_precision ,outputs, p_labels = baseline(creds_loader,\n","                                                                                                test_loader, \n","                                                                                                meta_loader,\n","                                                                                                clean=True)\n","\n","                        elif method == 'ground_truth':           # ground truth\n","                            hamming_loss, ranking_loss, one_error, coverage, avg_precision ,outputs, p_labels = baseline(clean_loader,\n","                                                                                                test_loader, \n","                                                                                                meta_loader,\n","                                                                                                clean=False)\n","\n","                        #np.savetxt(f\"{dir}/result/myfunc_with_regu/{data}/{str(p_noise)}/true/{str(p_true)}/creds_{str(i)}.csv\",outputs,delimiter =',')\n","                        #np.savetxt(f\"{dir}/result/myfunc_with_regu/{data}/{str(p_noise)}/true/{str(p_true)}/predict_labels_{str(i)}.csv\",p_labels,fmt='%d',delimiter=',')\n","\n","\n","                        print()\n","                        print('Test results of the last :\\t',\n","                            'Best hLoss: {:.4f} '.format(hamming_loss),\n","                            'Best rLoss: {:.4f} '.format(ranking_loss),\n","                            'Best oError: {:.4f} '.format(one_error),\n","                            'Best conv: {:.4f} '.format(coverage),\n","                            'Best avgPre: {:.4f} '.format(avg_precision))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":255,"status":"ok","timestamp":1673512327280,"user":{"displayName":"水口晴陽","userId":"02563633021533009649"},"user_tz":-540},"id":"WIWYUnPZE88E","outputId":"7b8bd9d6-7d81-4005-bbd4-026f3ff9b9a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Jan 12 08:32:09 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   49C    P0    28W /  70W |      3MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"FpLer4UfWrwQ"},"source":["next from mirflickr cv=1 p_true=3"]},{"cell_type":"markdown","metadata":{"id":"IA8KJMPh4CWw"},"source":["emotions\n","\n","Best hLoss: 0.2927  Best rLoss: 0.2216  Best oError: 0.3445  Best conv: 0.3697  Best avgPre: 0.7530 p_noise = 9 meta\n","\n","Best hLoss: 0.2927  Best rLoss: 0.2724  Best oError: 0.3697  Best conv: 0.4062  Best avgPre: 0.7173 p_noise = 9 baseline_clean\n","\n","Best hLoss: 0.2871  Best rLoss: 0.2653  Best oError: 0.3613  Best conv: 0.4104  Best avgPre: 0.7217 p_noise = 3 baseline_clean\n","\n","Best hLoss: 0.3235  Best rLoss: 0.2911  Best oError: 0.4202  Best conv: 0.4370  Best avgPre: 0.6867 p_noise = 3 meta"]},{"cell_type":"markdown","metadata":{"id":"JeAFu09Xgfch"},"source":[" music_emotion\n","\n"," Best hLoss: 0.2060  Best rLoss: 0.2424  Best oError: 0.4594  Best conv: 0.4083  Best avgPre: 0.6124 \n","\n"," Best hLoss: 0.1977  Best rLoss: 0.2524  Best oError: 0.4682  Best conv: 0.4193  Best avgPre: 0.6173 batch = 50 lr = 0.1\n","\n"," Best hLoss: 0.1958  Best rLoss: 0.2513  Best oError: 0.4660  Best conv: 0.4164  Best avgPre: 0.6181 batch = meta_size lr = 0.01\n","\n"," Best hLoss: 0.1855  Best rLoss: 0.1984  Best oError: 0.3855  Best conv: 0.3608  Best avgPre: 0.6708 ground_truth\n","\n"," Best hLoss: 0.1930  Best rLoss: 0.2104  Best oError: 0.4148  Best conv: 0.3705  Best avgPre: 0.6594 ground_truth with relu layer\n","\n"," Best hLoss: 0.1940  Best rLoss: 0.2204  Best oError: 0.4331  Best conv: 0.3874  Best avgPre: 0.6442 p_true = 9 baseline_clean\n","\n"," Best hLoss: 0.2044  Best rLoss: 0.2387  Best oError: 0.4674  Best conv: 0.4021  Best avgPre: 0.6132 p_true = 9 baseline_clean creds = plabels\n","\n"," Best hLoss: 0.2158  Best rLoss: 0.2273  Best oError: 0.4206  Best conv: 0.3900  Best avgPre: 0.6430 p_true = 9 meta\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1V_Xq4SOO7kk"},"outputs":[],"source":["dir = \"/content/drive/MyDrive/Colab Notebooks\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671862288185,"user":{"displayName":"水口晴陽","userId":"02563633021533009649"},"user_tz":-540},"id":"Zu2zdGOASAv2","outputId":"56f5776f-e8b8-45bc-b08f-98555390a067"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/result\n"]}],"source":["%cd /content/drive/MyDrive/Colab Notebooks/result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17097,"status":"ok","timestamp":1671862325318,"user":{"displayName":"水口晴陽","userId":"02563633021533009649"},"user_tz":-540},"id":"X8TZam6qPVwI","outputId":"774db575-e8c2-40ad-c3b9-9ff6e987eea1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/result/meta-ver2\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/music_emotion\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/music_emotion/0\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/music_emotion/0/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/music_emotion/0\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/music_emotion\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/music_style\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/music_style/0\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/music_style/0/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/music_style/0\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/music_style\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/mirflickr\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/mirflickr/0\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/mirflickr/0/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/mirflickr/0\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/mirflickr\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/YeastBP\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/YeastBP/0\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/YeastBP/0/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/YeastBP/0\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/YeastBP\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions/50\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions/50/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions/50\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions/100\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions/100/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions/100\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions/150\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions/150/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions/150\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/emotions\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron/50\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron/50/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron/50\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron/100\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron/100/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron/100\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron/150\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron/150/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron/150\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/enron\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500/50\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500/50/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500/50\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500/100\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500/100/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500/100\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500/150\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500/150/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500/150\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/CAL500\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene/50\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene/50/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene/50\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene/100\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene/100/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene/100\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene/150\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene/150/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene/150\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/scene\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase/50\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase/50/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase/50\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase/100\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase/100/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase/100\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase/150\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase/150/true\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase/150\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2/genbase\n","/content/drive/MyDrive/Colab Notebooks/result/meta-ver2\n","/content/drive/MyDrive/Colab Notebooks/result\n"]}],"source":["#methods = ['meta', 'meat_clean', 'baseline', 'baseline_clean', 'ground_truth']\n","methods = ['meta-ver2']\n","datasetss = ['music_emotion', 'music_style','mirflickr','YeastBP','emotions','enron','CAL500','scene','genbase']\n","noise_list = [50,100,150]\n","#noise_list = [50]\n","true_list = [1,3,5,7,9]\n","for method in methods:\n","  %mkdir {method}\n","  %cd {method}\n","  for data in datasetss:\n","    if data in ['music_emotion', 'music_style','mirflickr','YeastBP']:\n","      %mkdir {data}\n","      %cd {data}\n","      for n in [0]:\n","        %mkdir {str(n)}\n","        %cd {str(n)}\n","        %mkdir true\n","        %cd true\n","        for t in true_list:\n","          %mkdir {str(t)}\n","        %cd ..\n","        %cd ..\n","    else:\n","      %mkdir {data}\n","      %cd {data}\n","      for n in noise_list:\n","        %mkdir {str(n)}\n","        %cd {str(n)}\n","        %mkdir true\n","        %cd true\n","        for t in true_list:\n","          %mkdir {str(t)}\n","        %cd ..\n","        %cd ..\n","    %cd ..\n","  %cd .."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRbusio_mZfW"},"outputs":[],"source":["from scipy.sparse import csr_matrix\n","from sklearn.metrics import hamming_loss, coverage_error, label_ranking_average_precision_score, label_ranking_loss\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGxWBHadk8gg"},"outputs":[],"source":["d_list=[\"emotions\",\"enron\",\"CAL500\",\"scene\",\"genbase\",\"mirflickr\",\"music_style\",\"music_emotion\"]\n","for i in range(len(d_list)):\n","  dataname=d_list[i]\n","  ath = f\"/content/drive/MyDrive/Colab Notebooks/new_data2/\" + dataname + \"/\"\n","  labels = np.loadtxt(ath+\"target.csv\", delimiter=',',dtype =float)\n","  if dataname in [\"mirflickr\",\"music_style\",\"music_emotion\"]:\n","    plabels = np.loadtxt(ath+\"cand/0.csv\", delimiter=',',dtype =float)\n","  else:\n","    plabels = np.loadtxt(ath+\"cand/50.csv\", delimiter=',',dtype =float)\n","  a,b=np.shape(labels)\n","  z=np.zeros((a,b))\n","  o=np.ones((a,b))\n","  zl=coverage_error(labels,z)\n","  ol=coverage_error(labels,o)\n","  print(dataname)\n","  print(\"zl=\"+str(zl))\n","  print(\"ol=\"+str(ol))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1673402919386,"user":{"displayName":"水口晴陽","userId":"02563633021533009649"},"user_tz":-540},"id":"5vCUOPRumsAW","outputId":"9768974f-814b-4330-dbb0-06598152cc21"},"outputs":[{"name":"stdout","output_type":"stream","text":["(470, 6)\n","(470, 6)\n"]}],"source":["a = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/result/predict_creds/PML-MD/emotions/50/true/1/creds_0.csv\",delimiter = \",\")\n","ba = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/result/predict_creds/PML-MD/emotions/50/true/1/target_0.csv\",delimiter = \",\")\n","print(np.shape(a))\n","print(np.shape(ba))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMN5+nIzVUoYwi9cn72AQeK","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"53d3b87c0569ee8c7d27957a82fa406e7c5656d8e6d222384317b49a318c1993"}}},"nbformat":4,"nbformat_minor":0}
